# ========================== index_build.py â€” START ===========================
# [00] IMPORTS & GLOBALS â€” START
from __future__ import annotations
import os, io, json, gzip, shutil, zipfile, time
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Tuple

# ì™¸ë¶€
import streamlit as st  # type: ignore

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ (ì„œë¹„ìŠ¤ê³„ì • ì „ìš©)
from google.oauth2 import service_account
from googleapiclient.discovery import build as gbuild
from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload

# ë‚´ë¶€ ì˜ì¡´ (ê²½ë¡œ/ì„¤ì •)
try:
    from src.config import PERSIST_DIR as _PERSIST_DIR, APP_DATA_DIR as _APP_DATA_DIR
except Exception:
    _PERSIST_DIR = str(Path.home() / ".maic" / "persist")
    _APP_DATA_DIR = str(Path.home() / ".maic")

PERSIST_DIR = Path(_PERSIST_DIR).expanduser()
BACKUP_DIR  = (Path(_APP_DATA_DIR).expanduser() / "backup")
PERSIST_DIR.mkdir(parents=True, exist_ok=True)
BACKUP_DIR.mkdir(parents=True, exist_ok=True)
# [00] IMPORTS & GLOBALS â€” END


# [01] UTILS â€” START
def _msg(update_msg: Callable[[str], None], s: str) -> None:
    try: update_msg(str(s))
    except Exception: pass

def _pct(update_pct: Callable[[int, Optional[str]], None], v: int, msg: Optional[str] = None) -> None:
    try: update_pct(int(v), msg)
    except Exception: pass

def _now_tag() -> str:
    return time.strftime("%Y%m%d-%H%M%S", time.localtime())

def _read_secret(key: str, default: Optional[str] = None) -> Optional[str]:
    try:
        val = st.secrets.get(key)  # type: ignore[attr-defined]
        if val is None:
            return os.getenv(key, default)
        if isinstance(val, (str,)):
            return val
        return json.dumps(val, ensure_ascii=False)
    except Exception:
        return os.getenv(key, default)
# [01] UTILS â€” END


# [02] GOOGLE DRIVE AUTH (SERVICE ACCOUNT ONLY) â€” START
def _drive_client():
    """
    ì„œë¹„ìŠ¤ê³„ì • ì „ìš© í´ë¼ì´ì–¸íŠ¸.
    - secrets['gcp_service_account']ë¥¼ ë¬¸ìì—´(JSON) ë˜ëŠ” ê°ì²´ë¡œ í—ˆìš©
    - ìŠ¤ì½”í”„: read-only
    """
    raw = st.secrets.get("gcp_service_account") if hasattr(st, "secrets") else None  # type: ignore
    if isinstance(raw, str):
        info = json.loads(raw)
    elif isinstance(raw, dict):
        info = dict(raw)
    else:
        raise RuntimeError("gcp_service_account ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤.")

    scopes = st.secrets.get("GDRIVE_SCOPES") if hasattr(st, "secrets") else None  # type: ignore
    if isinstance(scopes, str):
        try:
            scopes = json.loads(scopes)
        except Exception:
            scopes = [scopes]
    if not scopes:
        scopes = [
            "https://www.googleapis.com/auth/drive.readonly",
            "https://www.googleapis.com/auth/drive.metadata.readonly",
        ]

    creds = service_account.Credentials.from_service_account_info(info, scopes=scopes)
    return gbuild("drive", "v3", credentials=creds, cache_discovery=False)
# [02] GOOGLE DRIVE AUTH â€” END


# [03] DRIVE HELPERS â€” START
def _find_folder_id(kind: str, *, fallback: Optional[str] = None) -> Optional[str]:
    """
    kind: "PREPARED" | "BACKUP" | "DEFAULT"
    ê¸°ë³¸ì€ ì‹œí¬ë¦¿ í‚¤ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ fallback.
    """
    key = {
        "PREPARED": "GDRIVE_PREPARED_FOLDER_ID",
        "BACKUP":   "GDRIVE_BACKUP_FOLDER_ID",
        "DEFAULT":  "GDRIVE_PREPARED_FOLDER_ID",
    }.get(kind.upper(), "GDRIVE_PREPARED_FOLDER_ID")
    return _read_secret(key, fallback)

def _list_files_in_folder(svc, folder_id: str) -> List[Dict[str, Any]]:
    q = f"'{folder_id}' in parents and trashed=false"
    fields = "files(id, name, mimeType, modifiedTime, size, md5Checksum)"
    res = svc.files().list(q=q, fields=fields, pageSize=1000).execute()
    return res.get("files", [])

def _download_file_bytes(svc, file_id: str) -> bytes:
    req = svc.files().get_media(fileId=file_id)
    buf = io.BytesIO()
    downloader = MediaIoBaseDownload(buf, req)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    return buf.getvalue()
# [03] DRIVE HELPERS â€” END


# [04] TEXT & CHUNK BUILDERS (PLACEHOLDER / YOUR PIPELINE) â€” START
def _extract_text_from_bytes(name: str, data: bytes) -> str:
    """
    ì‹¤ì œ êµ¬í˜„ì²´ì— ë§ê²Œ êµì²´í•˜ì„¸ìš”.
    ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ UTF-8 ë””ì½”ë”© ì‹œë„ -> ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´.
    """
    try:
        return data.decode("utf-8", errors="ignore")
    except Exception:
        return ""

def _to_chunks(name: str, text: str, meta: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ì‹¤ì œ ì²­í¬ ë¶„í•´ ê·œì¹™ì— ë§ê²Œ êµì²´í•˜ì„¸ìš”.
    ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì¤„ ë‹¨ìœ„ ë¶„í•´ ì˜ˆì‹œ.
    """
    out: List[Dict[str, Any]] = []
    for i, line in enumerate(text.splitlines()):
        line = line.strip()
        if not line:
            continue
        out.append({
            "text": line,
            "meta": {
                **meta,
                "line_index": i,
            }
        })
    return out
# [04] TEXT & CHUNK BUILDERS â€” END


# [05] LOCAL STORE HELPERS â€” START
def _persist_write_json(obj: Any, path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")

def _persist_write_jsonl(rows: Iterable[Dict[str, Any]], path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def _gzip_file(src: Path, dst: Optional[Path] = None, compresslevel: int = 6) -> Path:
    if dst is None:
        dst = src.with_suffix(src.suffix + ".gz")
    with src.open("rb") as fr, gzip.open(dst, "wb", compresslevel=compresslevel) as fw:
        shutil.copyfileobj(fr, fw)
    return dst
# [05] LOCAL STORE HELPERS â€” END


# [06] QUALITY REPORT (CURRENT SCHEMA) â€” START
def _quality_report(manifest: Dict[str, Any], *, extra_counts: Dict[str, Any]) -> Dict[str, Any]:
    """
    í˜„ì¬ ì²­í¬ êµ¬ì¡°(meta.*)ì— ë§ì¶˜ ê°„ë‹¨ ë¦¬í¬íŠ¸.
    - ë¬¸ì„œ ìˆ˜, ì²­í¬ ìˆ˜, íŒŒì¼ëª…/íŒŒì¼ID ë¶„í¬, meta.page_approx ìœ ë¬´ ë“±
    """
    docs = manifest.get("docs", [])
    chunks = manifest.get("chunks", [])
    doc_count = len(docs)
    chunk_count = len(chunks)

    meta_fields = {
        "file_name": 0,
        "file_id": 0,
        "mimeType": 0,
        "page_approx": 0,
    }
    for c in chunks:
        m = c.get("meta", {})
        if "file_name" in m: meta_fields["file_name"] += 1
        if "file_id"   in m: meta_fields["file_id"]   += 1
        if "mimeType"  in m: meta_fields["mimeType"]  += 1
        if "page_approx" in m: meta_fields["page_approx"] += 1

    return {
        "stats": {
            "documents": doc_count,
            "chunks": chunk_count,
            **{f"meta_has_{k}": v for k, v in meta_fields.items()},
        },
        "extra": extra_counts or {},
    }
# [06] QUALITY REPORT â€” END


# [07] BACKUP ZIP (DRIVE-AGNOSTIC LOCAL + (ì„ íƒ)REMOTE) â€” START
def _make_and_upload_backup_zip(svc, backup_folder_id: Optional[str]) -> Optional[str]:
    """
    ë¡œì»¬ ë°±ì—… zip ìƒì„± í›„, (ìˆë‹¤ë©´) Driveì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    GitHub Releases ë°±ì—… ì •ì±…ì€ ë³„ë„ ëª¨ë“ˆ(src.backup.github_release)ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    ts = _now_tag()
    zip_path = BACKUP_DIR / f"maic_backup_{ts}.zip"
    with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as z:
        for p in PERSIST_DIR.rglob("*"):
            if p.is_file():
                z.write(p, arcname=p.relative_to(PERSIST_DIR))
        z.writestr(".backup_info.txt", json.dumps({"created_at": ts}, ensure_ascii=False))
    uploaded_id = None
    try:
        if svc and backup_folder_id:
            media = MediaIoBaseUpload(
                io.BytesIO(zip_path.read_bytes()), mimetype="application/zip", resumable=True
            )
            meta = {"name": zip_path.name, "parents": [backup_folder_id]}
            f = svc.files().create(body=meta, media_body=media, fields="id").execute()
            uploaded_id = f.get("id")
    except Exception as e:
        # ì—…ë¡œë“œ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ â€” ë¡œì»¬ zipë§Œ ìˆì–´ë„ OK
        print(f"[backup][warn] drive upload failed: {type(e).__name__}: {e}")
    return uploaded_id
# [07] BACKUP ZIP â€” END


# [08] BUILD PIPELINE CORE â€” START
def _build_from_prepared(svc, prepared_folder_id: str) -> Tuple[int, int, Dict[str, Any], Dict[str, Any]]:
    """
    prepared í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ ìŠ¤ìº” â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ì²­í¬ ìƒì„± â†’ manifest/chunks ë°˜í™˜.
    ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë³€ê²½ ê°ì§€(delta)Â·MIMEë³„ ì²˜ë¦¬Â·PDF í˜ì´ì§€ìˆ˜ ê³„ì‚° ë“±ì„ ì ìš©í•˜ì„¸ìš”.
    """
    files = _list_files_in_folder(svc, prepared_folder_id)
    docs_summary: List[Dict[str, Any]] = []
    all_chunks: List[Dict[str, Any]] = []

    for f in files:
        fid = f["id"]
        name = f.get("name", fid)
        mime = f.get("mimeType", "")
        data = _download_file_bytes(svc, fid)
        text = _extract_text_from_bytes(name, data)
        meta = {
            "file_id": fid,
            "file_name": name,
            "mimeType": mime,
            "page_approx": None,  # í•„ìš” ì‹œ ë¹ ë¥¸ í˜ì´ì§€ìˆ˜ ì¶”ì • ë¡œì§ ì—°ê²°
        }
        chunks = _to_chunks(name, text, meta)
        if chunks:
            all_chunks.extend(chunks)
        docs_summary.append({
            "id": fid, "name": name, "mimeType": mime, "size": f.get("size"), "md5": f.get("md5Checksum")
        })

    manifest = {
        "built_at": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
        "docs": docs_summary,
        "stats": {"documents": len(docs_summary), "chunks": len(all_chunks)},
    }
    extra = {"processed_files": len(docs_summary), "generated_chunks": len(all_chunks)}
    return len(docs_summary), len(all_chunks), manifest, extra
# [08] BUILD PIPELINE CORE â€” END


# [09] FOLDER RESOLUTION â€” START
def _resolve_ids(svc, gdrive_folder_id: str) -> Tuple[str, str]:
    prepared_id = _find_folder_id("PREPARED", fallback=gdrive_folder_id)
    backup_id   = _find_folder_id("BACKUP") or prepared_id
    if not prepared_id:
        raise KeyError("prepared í´ë” IDë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return prepared_id, backup_id
# [09] FOLDER RESOLUTION â€” END


# [10] PUBLIC ENTRY (ë¹Œë“œ ì‹¤í–‰; ì„œë¹„ìŠ¤ê³„ì • + GitHub Release ì—…ë¡œë“œ) â€” START
def build_index_with_checkpoint(
    update_pct: Callable[[int, Optional[str]], None],
    update_msg: Callable[[str], None],
    gdrive_folder_id: str,
    gcp_creds: Mapping[str, object],          # í˜¸í™˜ì„± ìœ ì§€ìš© (ë¯¸ì‚¬ìš©)
    persist_dir: str,                          # í˜¸í™˜ì„± ìœ ì§€ìš© (ì‚¬ìš©í•˜ë˜ ê¸°ë³¸ì€ PERSIST_DIR)
    remote_manifest: Dict[str, Dict[str, object]],
    should_stop: Optional[Callable[[], bool]] = None,
) -> Dict[str, Any]:

    # ë©”ì‹œì§€/í¼ì„¼íŠ¸ í—¬í¼
    def pct(v: int, m: Optional[str] = None): _pct(update_pct, v, m)
    def msg(s: str): _msg(update_msg, s)

    # ì¤€ë¹„
    msg("ğŸ” Connecting Google Drive (service account)â€¦")
    svc = _drive_client()
    pct(5, "drive-ready")

    prepared_id, backup_id = _resolve_ids(svc, gdrive_folder_id)

    # ë¹Œë“œ
    msg("ğŸ“¦ Scanning prepared folder and building chunksâ€¦")
    processed, chunks, manifest, stats = _build_from_prepared(svc, prepared_id)
    pct(70, f"processed={processed}, chunks={chunks}")

    if should_stop and should_stop():
        return {"ok": False, "stopped": True}

    # ë¡œì»¬ ì €ì¥
    msg("ğŸ§® Writing manifest/chunks locallyâ€¦")
    out_dir = PERSIST_DIR
    out_dir.mkdir(parents=True, exist_ok=True)
    manifest_path = out_dir / "manifest.json"
    chunks_path   = out_dir / "chunks.jsonl"
    _persist_write_json(manifest, manifest_path)
    _persist_write_jsonl(manifest.get("chunks", []) or [], chunks_path)  # í˜„ì¬ ì˜ˆì‹œì—ì„œëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸

    # í’ˆì§ˆ ë¦¬í¬íŠ¸
    msg("ğŸ“Š Building quality reportâ€¦")
    report = _quality_report({"docs": manifest.get("docs", []), "chunks": manifest.get("chunks", []) or []}, extra_counts=stats)
    pct(85, "report-ready")

    # ë¡œì»¬/ë“œë¼ì´ë¸Œ ë°±ì—… ZIP
    msg("â¬†ï¸ Uploading backup zipâ€¦")
    uploaded_id = _make_and_upload_backup_zip(svc, backup_id)
    pct(92, "backup-zip-uploaded")

    # GitHub Releases ì—…ë¡œë“œ (í•­ìƒ .gz ìƒì„± í›„ ì—…ë¡œë“œ)
    try:
        gz_path = _gzip_file(chunks_path)  # chunks.jsonl.gz ìƒì„±/ê°±ì‹ 
        from src.backup.github_release import upload_index_release
        msg("ğŸš€ Publishing index to GitHub Releasesâ€¦")
        res = upload_index_release(
            manifest_path=manifest_path,
            chunks_jsonl_path=chunks_path,  # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ .gz ìƒì„±í•˜ì§€ë§Œ, ì´ë¯¸ ìš°ë¦¬ê°€ ìƒì„±í–ˆìŒ
            include_zip=False,
            keep=2,
            build_meta={
                "processed_files": processed,
                "generated_chunks": chunks,
                "prepared_folder_id": prepared_id,
            },
        )
        msg(f"âœ… GitHub Releases ì™„ë£Œ: {res.get('tag')} / {res.get('assets')}")
    except Exception as e:
        msg(f"âš ï¸ GitHub ì—…ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")

    pct(100, "done")

    return {
        "ok": True,
        "processed_files": processed,
        "generated_chunks": chunks,
        "stats": stats,
        "report": report,
        "backup_zip_id": uploaded_id,
        "prepared_folder_id": prepared_id,
        "backup_folder_id": backup_id,
        "auth_mode": "service-account",
        "persist_dir": str(PERSIST_DIR),
    }
# [10] PUBLIC ENTRY â€” END
# =========================== index_build.py â€” END ============================
