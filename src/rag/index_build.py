# ===== [01] IMPORTS & CONSTANTS =============================================
from __future__ import annotations

import io, os, json, time, hashlib, zipfile
from pathlib import Path
from typing import Callable, Dict, Mapping, Any, List, Tuple, Optional

import streamlit as st

REQ_FILES = ["chunks.jsonl", "manifest.json", "quality_report.json"]

# ë¡œì»¬ ì €ì¥ ê²½ë¡œ ê¸°ë³¸ê°’ (src/configê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©)
try:
    from src.config import APP_DATA_DIR as _APP
    from src.config import PERSIST_DIR as _PERSIST
    from src.config import MANIFEST_PATH as _MANIFEST
    from src.config import QUALITY_REPORT_PATH as _QRP
    APP_DATA_DIR = Path(_APP)
    PERSIST_DIR = Path(_PERSIST)
    MANIFEST_PATH = Path(_MANIFEST)
    QUALITY_REPORT_PATH = Path(_QRP)
except Exception:
    APP_DATA_DIR = Path(os.getenv("APP_DATA_DIR") or (Path.home() / ".maic"))
    PERSIST_DIR = APP_DATA_DIR / "persist"
    MANIFEST_PATH = APP_DATA_DIR / "manifest.json"
    QUALITY_REPORT_PATH = APP_DATA_DIR / "quality_report.json"

PERSIST_DIR.mkdir(parents=True, exist_ok=True)
APP_DATA_DIR.mkdir(parents=True, exist_ok=True)


# ===== [02] SECRETS & AUTH (USER OAUTH ìš°ì„ , SA í´ë°±) ========================
# - ê°œì¸ìš© ë“œë¼ì´ë¸Œ: OAuth ì‚¬ìš©ì í† í°ìœ¼ë¡œ ì ‘ê·¼
#   í•„ìˆ˜ í‚¤: GDRIVE_OAUTH_CLIENT_ID, GDRIVE_OAUTH_CLIENT_SECRET, GDRIVE_OAUTH_REFRESH_TOKEN
# - ì—†ìœ¼ë©´ ì„œë¹„ìŠ¤ê³„ì • JSONì„ ì‚¬ìš©(ê³µìœ ë¡œ ê¶Œí•œ ë¶€ì—¬ëœ í´ë”ë§Œ ì ‘ê·¼ ê°€ëŠ¥)

def _flatten_secrets(obj: Any, prefix: str = "") -> List[Tuple[str, Any]]:
    out: List[Tuple[str, Any]] = []
    try:
        from collections.abc import Mapping as _Map
        if isinstance(obj, _Map):
            for k, v in obj.items():
                p = f"{prefix}.{k}" if prefix else str(k)
                out.extend(_flatten_secrets(v, p))
        else:
            out.append((prefix, obj))
    except Exception:
        out.append((prefix, obj))
    return out

def _get_drive_credentials():
    """OAuth ì‚¬ìš©ì ìê²©ì„ ë¨¼ì € ì‹œë„, ì‹¤íŒ¨ ì‹œ ì„œë¹„ìŠ¤ê³„ì •ìœ¼ë¡œ í´ë°±."""
    # 1) USER OAUTH (ê°œì¸ ë“œë¼ì´ë¸Œ)
    cid   = st.secrets.get("GDRIVE_OAUTH_CLIENT_ID") or st.secrets.get("GOOGLE_OAUTH_CLIENT_ID")
    csec  = st.secrets.get("GDRIVE_OAUTH_CLIENT_SECRET") or st.secrets.get("GOOGLE_OAUTH_CLIENT_SECRET")
    r_tok = st.secrets.get("GDRIVE_OAUTH_REFRESH_TOKEN") or st.secrets.get("GOOGLE_OAUTH_REFRESH_TOKEN")
    t_uri = st.secrets.get("GDRIVE_OAUTH_TOKEN_URI") or "https://oauth2.googleapis.com/token"
    if cid and csec and r_tok:
        try:
            from google.oauth2.credentials import Credentials as UserCredentials
            creds = UserCredentials(
                None,
                refresh_token=str(r_tok),
                client_id=str(cid),
                client_secret=str(csec),
                token_uri=str(t_uri),
                scopes=["https://www.googleapis.com/auth/drive"],
            )
            return creds
        except Exception as e:
            # OAuth ìê²© ìƒì„± ì‹¤íŒ¨ ì‹œ, ì„œë¹„ìŠ¤ê³„ì •ìœ¼ë¡œ í´ë°± ì‹œë„
            pass

    # 2) SERVICE ACCOUNT (ê³µìœ  í•„ìš”)
    #    í”í•œ í‚¤ë“¤ ë¨¼ì € ë³¸ ë’¤, ì „ì²´ ì‹œí¬ë¦¿ì„ í›‘ì–´ service_account JSON ìë™ íƒìƒ‰
    candidates = (
        "GDRIVE_SERVICE_ACCOUNT_JSON",
        "GOOGLE_SERVICE_ACCOUNT_JSON",
        "SERVICE_ACCOUNT_JSON",
        "gdrive_service_account_json",
        "service_account_json",
        "GCP_SERVICE_ACCOUNT",
        "gcp_service_account",
    )
    raw = None
    for k in candidates:
        if k in st.secrets and str(st.secrets[k]).strip():
            raw = st.secrets[k]; break
    if raw is None:
        for _, v in _flatten_secrets(st.secrets):
            try:
                from collections.abc import Mapping as _Map
                if isinstance(v, _Map) and v.get("type") == "service_account" and "client_email" in v and "private_key" in v:
                    raw = v; break
                if isinstance(v, str) and '"type": "service_account"' in v:
                    raw = v; break
            except Exception:
                pass
    if raw is None:
        raise KeyError(
            "Drive ìê²©ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
            "(OAuth: GDRIVE_OAUTH_CLIENT_ID/SECRET/REFRESH_TOKEN ë˜ëŠ” "
            "Service Account JSON ì¤‘ í•˜ë‚˜ê°€ í•„ìš”)"
        )

    info = json.loads(raw) if isinstance(raw, str) else dict(raw)
    from google.oauth2.service_account import Credentials as SACredentials
    return SACredentials.from_service_account_info(info, scopes=["https://www.googleapis.com/auth/drive"])

def _find_folder_id(kind: str, fallback: Optional[str] = None) -> Optional[str]:
    """
    kind: 'PREPARED' | 'BACKUP' | 'DEFAULT'
    - PREPARED: ìˆ˜ì—…ìë£Œ/ë¬¸ë²•ì„œê°€ ìˆëŠ” í´ë” (my-ai-teacher-data/prepared)
    - BACKUP  : ìµœì í™” í›„ ì—…ë¡œë“œí•˜ëŠ” ZIP ë³´ê´€ í´ë” (my-ai-teacher-data/backup_zip)
    - DEFAULT : ì¼ë°˜ ê¸°ë³¸ í´ë” ID
    """
    key_sets = {
        "PREPARED": ("GDRIVE_PREPARED_FOLDER_ID", "PREPARED_FOLDER_ID"),
        "BACKUP": ("GDRIVE_BACKUP_FOLDER_ID", "BACKUP_FOLDER_ID"),
        "DEFAULT": ("GDRIVE_FOLDER_ID",),
    }
    for key in key_sets.get(kind, ()):
        if key in st.secrets and str(st.secrets[key]).strip():
            return str(st.secrets[key]).strip()

    # ì¤‘ì²© ì„¹ì…˜ ì•ˆê¹Œì§€ ì „ìˆ˜ì¡°ì‚¬
    for path, val in _flatten_secrets(st.secrets):
        if isinstance(val, (str, int)) and "FOLDER_ID" in path.upper() and str(val).strip():
            up = path.upper()
            if kind == "PREPARED" and "PREPARED" in up:
                return str(val).strip()
            if kind == "BACKUP" and "BACKUP" in up:
                return str(val).strip()
            if kind == "DEFAULT" and "GDRIVE_FOLDER_ID" in up:
                return str(val).strip()
    return fallback


# ===== [03] DRIVE CLIENT & FILE LIST ========================================
def _drive_client():
    """ì‚¬ìš©ì OAuth ë˜ëŠ” ì„œë¹„ìŠ¤ê³„ì •ìœ¼ë¡œ Drive í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±."""
    try:
        from googleapiclient.discovery import build
    except Exception as e:
        raise RuntimeError("google-api-python-client íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
    creds = _get_drive_credentials()
    return build("drive", "v3", credentials=creds, cache_discovery=False)

def _list_files(service, folder_id: str) -> List[Dict[str, Any]]:
    q = f"'{folder_id}' in parents and trashed=false"
    fields = "files(id,name,mimeType,modifiedTime,md5Checksum,size),nextPageToken"
    files, token = [], None
    while True:
        resp = service.files().list(q=q, fields=fields, pageToken=token, pageSize=1000).execute()
        files.extend(resp.get("files", []))
        token = resp.get("nextPageToken")
        if not token:
            break
    files.sort(key=lambda x: x.get("name", ""))
    return files

def _find_latest_zip(service, folder_id: str):
    q = f"'{folder_id}' in parents and trashed=false and mimeType='application/zip'"
    resp = service.files().list(q=q, orderBy="modifiedTime desc",
                                fields="files(id,name,modifiedTime,size)", pageSize=1).execute()
    f = resp.get("files", [])
    return f[0] if f else None

def _download_file_bytes(service, file_id: str) -> bytes:
    from googleapiclient.http import MediaIoBaseDownload
    req = service.files().get_media(fileId=file_id)
    buf = io.BytesIO()
    done = False
    downloader = MediaIoBaseDownload(buf, req)
    while not done:
        _, done = downloader.next_chunk()
    buf.seek(0)
    return buf.read()

def _download_file_to(service, file_id: str, out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_bytes(_download_file_bytes(service, file_id))

def _upload_zip(service, folder_id: str, path: Path, name: str) -> str:
    from googleapiclient.http import MediaFileUpload
    media = MediaFileUpload(str(path), mimetype="application/zip", resumable=False)
    meta = {"name": name, "parents": [folder_id], "mimeType": "application/zip"}
    created = service.files().create(body=meta, media_body=media, fields="id").execute()
    return created.get("id")


# ===== [04] CONTENT EXTRACTORS (TXT/MD/GOOGLE DOCS/PDF; with cleaning) ======
GOOGLE_DOC = "application/vnd.google-apps.document"
TEXT_LIKE = {"text/plain", "text/markdown", "application/json"}
PDF_MIME  = "application/pdf"

def _clean_text_common(s: str) -> str:
    """ë¬¸ë‹¨/ì¤„ë°”ê¿ˆ/ê³µë°± ì •ë¦¬: í•˜ì´í”ˆ ì¤„ë°”ê¿ˆ, ì¤‘ë³µ ê³µë°±, ë¹ˆ ì¤„ ì œê±°."""
    import re
    if not s:
        return ""
    # 1) ì¤„ë§ í•˜ì´í”ˆ ì œê±°: "exam-\nple" â†’ "example"
    s = re.sub(r"(?<=\w)-\s*\n\s*(?=\w)", "", s)
    # 2) ë‹¨ì–´ ì¤‘ê°„ ë¶ˆí•„ìš” ê°œí–‰ ì œê±°: "ab\ncd" â†’ "ab cd"
    s = re.sub(r"(?<=\S)\n(?=\S)", " ", s)
    # 3) ì—¬ëŸ¬ ì—°ì† ê³µë°±/íƒ­ ì¶•ì†Œ
    s = re.sub(r"[ \t]+", " ", s)
    # 4) í˜ì´ì§€ ë‹¨ìœ„ ë¹ˆ ì¤„ ì •ë¦¬
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def _extract_text(service, file: Dict[str, Any], stats: Dict[str, int]) -> Optional[str]:
    """
    íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•˜ê³  í´ë¦°ì—….
    ë°˜í™˜: ì •ì œëœ í…ìŠ¤íŠ¸(str) ë˜ëŠ” None(ë¯¸ì§€ì›/ì‹¤íŒ¨)
    """
    mime = file.get("mimeType")
    fid  = file["id"]

    # Google Docs â†’ export(text/plain)
    if mime == GOOGLE_DOC:
        try:
            data = service.files().export(fileId=fid, mimeType="text/plain").execute()
            stats["gdocs"] = stats.get("gdocs", 0) + 1
            return _clean_text_common(data.decode("utf-8", errors="ignore"))
        except Exception:
            stats["gdocs"] = stats.get("gdocs", 0) + 1
            return None

    # í…ìŠ¤íŠ¸ ê³„ì—´ â†’ ë°”ì´ë„ˆë¦¬ ë‹¤ìš´ë¡œë“œ í›„ decode
    if mime in TEXT_LIKE:
        b = _download_file_bytes(service, fid)
        stats["text_like"] = stats.get("text_like", 0) + 1
        return _clean_text_common(b.decode("utf-8", errors="ignore"))

    # PDF â†’ PyPDF ì‚¬ìš©(ì—†ìœ¼ë©´ ìŠ¤í‚µ)
    if mime == PDF_MIME:
        try:
            import pypdf  # ì„¤ì¹˜ í•„ìš”: pyproject.tomlì— pypdf ì¶”ê°€ë¨
            data = _download_file_bytes(service, fid)
            reader = pypdf.PdfReader(io.BytesIO(data))
            pages = []
            for p in reader.pages:
                try:
                    pages.append(p.extract_text() or "")
                except Exception:
                    pages.append("")
            txt = "\n\n".join(pages)
            stats["pdf_parsed"] = stats.get("pdf_parsed", 0) + 1
            return _clean_text_common(txt)
        except Exception:
            stats["pdf_skipped"] = stats.get("pdf_skipped", 0) + 1
            return None

    # ê·¸ ì™¸ëŠ” ìŠ¤í‚µ (docx ë“±ì€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™•ì¥)
    stats["others_skipped"] = stats.get("others_skipped", 0) + 1
    return None



# ===== [05] CHUNKING (paragraph-first, supports both overlap modes) ==========
from typing import Optional, List

def _norm_ws(s: str) -> str:
    return " ".join(s.split())

def _split_paragraphs(text: str) -> List[str]:
    """ë¹ˆ ì¤„/ê°œí–‰ ê¸°ë°˜ ë¬¸ë‹¨ ë¶„í• (ì§€ë‚˜ì¹œ ìª¼ê°œì§ ë°©ì§€)."""
    paras = [p.strip() for p in text.split("\n") if p.strip()]
    return paras

def _chunk_text(
    text: str,
    target_chars: int = 1000,        # ê¶Œì¥ 800~1200
    *,                                 # ì•„ë˜ ì¸ìëŠ” í‚¤ì›Œë“œ ì „ìš©(í˜¸í™˜ì„± ë³´ì¥)
    overlap: Optional[int] = None,     # ê¸€ì ìˆ˜ ê¸°ì¤€ ì˜¤ë²„ë©(ì˜ˆ: 120). ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ìš©.
    overlap_ratio: float = 0.12,       # ë¹„ìœ¨ ê¸°ì¤€ ì˜¤ë²„ë©(ì˜ˆ: 0.12 = 12%)
) -> List[str]:
    """
    ë¬¸ë‹¨ ìš°ì„  ì²­í¬ ë¶„í• .
    - overlap ì´ ì£¼ì–´ì§€ë©´ 'ê¸€ì ìˆ˜'ë¡œ ì ìš© (ê¸°ì¡´ í˜¸ì¶œê³¼ í˜¸í™˜: overlap=120)
    - overlap ì´ Noneì´ë©´ overlap_ratio(ë¹„ìœ¨)ë¡œ ì ìš©
    """
    if not text.strip():
        return []

    paras = _split_paragraphs(text)
    chunks: List[str] = []
    cur: List[str] = []
    cur_len = 0
    max_chars = max(400, int(target_chars))  # ìµœì†Œ í•˜í•œì„ 

    for p in paras:
        seg = p
        # í˜„ì¬ ì²­í¬ì— ë” ë¶™ì´ë©´ íƒ€ê²Ÿ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì˜ë¼ì„œ ë‚´ë³´ëƒ„
        if cur and cur_len + len(seg) + 1 > max_chars:
            joined = _norm_ws("\n".join(cur))
            chunks.append(joined)

            # --- ì†Œí”„íŠ¸ ì˜¤ë²„ë© ê³„ì‚° -----------------------------------------
            if overlap is not None:
                keep = int(max(0, min(overlap, len(joined))))
            else:
                keep = int(max(0, min(int(len(joined) * overlap_ratio), len(joined))))
            tail = joined[-keep:] if keep > 0 else ""
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ë²„í¼
            cur, cur_len = ([tail] if tail else []), len(tail)
            # ----------------------------------------------------------------

        cur.append(seg)
        cur_len += len(seg) + 1

    if cur:
        chunks.append(_norm_ws("\n".join(cur)))
    return chunks


# ===== [06] MANIFEST & DELTA =================================================
def _sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def _load_manifest(path: Path) -> Dict[str, Dict[str, Any]]:
    if path.exists():
        try:
            return json.loads(path.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def _save_manifest(path: Path, data: Dict[str, Dict[str, Any]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

def _need_update(prev: Dict[str, Any], now_meta: Dict[str, Any]) -> bool:
    if not prev:
        return True
    if prev.get("modifiedTime") != now_meta.get("modifiedTime"):
        return True
    if prev.get("md5Checksum") and now_meta.get("md5Checksum"):
        return prev["md5Checksum"] != now_meta["md5Checksum"]
    if prev.get("content_sha1") and now_meta.get("content_sha1"):
        return prev["content_sha1"] != now_meta["content_sha1"]
    return True


# ===== [07] BUILD FROM PREPARED =============================================
def _guess_section_hint(text: str) -> Optional[str]:
    """ê°„ë‹¨í•œ ì„¹ì…˜ íŒíŠ¸ ì¶”ì¶œ(ì²« ë¬¸ì¥/í—¤ë” ëŠë‚Œ). ì‹¤íŒ¨ ì‹œ None."""
    import re
    if not text:
        return None
    # ì¤„ ë‹¨ìœ„ë¡œ ë³´ê³ , ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸´ ê±´ ì œì™¸
    lines = [ln.strip() for ln in text.split("\n") if ln.strip()]
    for ln in lines[:5]:
        if 8 <= len(ln) <= 120:
            # ì „í˜•ì  ì„¹ì…˜ í”„ë¦¬í”½ìŠ¤ íŒ¨í„´
            if re.search(r"^(Chapter|Unit|Lesson|Section|Part)\s+[0-9IVX]+[:\-\.\s]", ln, re.I):
                return ln
            # ë¬¸ì¥ë¶€í˜¸ê°€ ì ê³  ì•ŒíŒŒê°€ ë§ì€ í—¤ë”ë¥˜
            if re.fullmatch(r"[A-Za-z0-9\s\-\(\)\.\,']{8,120}", ln) and sum(c.isalpha() for c in ln) > len(ln)*0.5:
                return ln
    # fallback: ì²« 80ì
    head = text.strip().split("\n", 1)[0]
    return (head[:80] + "â€¦") if len(head) > 80 else (head or None)


def _pdf_page_count_quick(service, file_id: str) -> Optional[int]:
    """PDF í˜ì´ì§€ ìˆ˜ë¥¼ ë¹ ë¥´ê²Œ ì¶”ì •(ë‹¤ìš´ë¡œë“œ í›„ pypdf ì‚¬ìš©). ì‹¤íŒ¨ ì‹œ None."""
    try:
        import pypdf
        data = _download_file_bytes(service, file_id)
        return len(pypdf.PdfReader(io.BytesIO(data)).pages)
    except Exception:
        return None


def _page_range_linear(total_len: int, pages: Optional[int], start: int, end: int) -> Optional[str]:
    """ë¬¸ì ê¸¸ì´ ë¹„ë¡€ë¡œ í˜ì´ì§€ ë²”ìœ„ë¥¼ ê·¼ì‚¬. pagesê°€ ì—†ìœ¼ë©´ None."""
    if not pages or total_len <= 0:
        return None
    # pos -> page(1-based)
    def pos2pg(pos: int) -> int:
        p = int((max(0, min(pos, total_len-1)) / max(1, total_len-1)) * (pages - 1)) + 1
        return max(1, min(p, pages))
    sp, ep = pos2pg(start), pos2pg(max(start, end-1))
    return f"{sp}" if sp == ep else f"{sp}â€“{ep}"


def _build_from_prepared(service, prepared_id: str) -> Tuple[int, int, Dict[str, Any], Dict[str, int]]:
    """
    prepared í´ë”ì˜ íŒŒì¼ë“¤ì„ ì½ì–´ chunks.jsonl/manifest.jsonì„ ê°±ì‹ í•œë‹¤.
    ë°˜í™˜: (processed_files, generated_chunks, manifest, stats)
    """
    files = _list_files(service, prepared_id)
    manifest = _load_manifest(MANIFEST_PATH)
    prev_ids = set(manifest.keys())
    out_path = PERSIST_DIR / "chunks.jsonl"

    stats: Dict[str, int] = {
        "gdocs": 0, "text_like": 0, "pdf_parsed": 0, "pdf_skipped": 0, "others_skipped": 0,
        "new_docs": 0, "updated_docs": 0, "unchanged_docs": 0
    }
    new_lines: List[str] = []
    processed, total_chunks = 0, 0
    changed_ids: set[str] = set()

    for f in files:
        meta_base = {
            "id": f["id"],
            "name": f.get("name"),
            "mimeType": f.get("mimeType"),
            "modifiedTime": f.get("modifiedTime"),
            "md5Checksum": f.get("md5Checksum"),
            "size": f.get("size"),
        }
        text = _extract_text(service, f, stats)
        if text is None or not text.strip():
            continue

        now_meta = {**meta_base, "content_sha1": _sha1(text)}
        prev_meta = manifest.get(f["id"], {})

        # ë³€ê²½ ë¶„ë¥˜(new/updated/unchanged)
        need = _need_update(prev_meta, now_meta)
        if not need:
            stats["unchanged_docs"] += 1
            continue
        if f["id"] not in prev_ids:
            stats["new_docs"] += 1
        else:
            stats["updated_docs"] += 1

        # ì²­í¬ ìƒì„±(+ ì˜¤ë²„ë© 120 ê³ ì •; ì•„ë˜ page_approx ê³„ì‚°ì— ì‚¬ìš©)
        chunks = _chunk_text(text, target_chars=1200, overlap=120)

        # PDFë©´ í˜ì´ì§€ ìˆ˜ë¥¼ ì–»ì–´ page_approx ê³„ì‚°ì— ì‚¬ìš©
        pages = _pdf_page_count_quick(service, f["id"]) if (f.get("mimeType") == "application/pdf") else None
        total_len = len(text)

        # ì„ í˜• ì˜¤í”„ì…‹ ëˆ„ì (ì˜¤ë²„ë© ê³ ë ¤)
        running_start = 0
        for i, ch in enumerate(chunks):
            start = running_start
            end = start + len(ch)
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  = í˜„ì¬ ì²­í¬ ê¸¸ì´ - keep(= overlap clip)
            keep = min(120, len(ch))
            running_start = end - keep

            rec = {
                "doc_id": f["id"],
                "doc_name": f.get("name"),
                "chunk_index": i,
                "text": ch,
                "meta": {
                    "file_id": f["id"],
                    "file_name": f.get("name"),
                    "source_drive_url": f"https://drive.google.com/file/d/{f['id']}/view",
                    "mime": f.get("mimeType"),
                    "modified": f.get("modifiedTime"),
                    "page_approx": _page_range_linear(total_len, pages, start, end),
                    "section_hint": _guess_section_hint(ch),
                },
            }
            new_lines.append(json.dumps(rec, ensure_ascii=False))

        now_meta["chunk_count"] = len(chunks)
        manifest[f["id"]] = now_meta

        processed += 1
        total_chunks += len(chunks)
        changed_ids.add(f["id"])

    # ê¸°ì¡´ ë¼ì¸ ë³´ì¡´ + ë³€ê²½ëœ doc_idì˜ ë¼ì¸ì€ ì œê±°
    existing: List[str] = []
    if out_path.exists():
        existing = [ln for ln in out_path.read_text(encoding="utf-8").splitlines() if ln.strip()]
    filtered_existing = []
    for ln in existing:
        try:
            obj = json.loads(ln)
            if obj.get("doc_id") in changed_ids:
                continue
        except Exception:
            pass
        filtered_existing.append(ln)

    merged = filtered_existing + new_lines
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("\n".join(merged) + ("\n" if merged else ""), encoding="utf-8")
    _save_manifest(MANIFEST_PATH, manifest)

    return processed, total_chunks, manifest, stats


# ===== [08] QUALITY REPORT & BACKUP ZIP =====================================
def _percentiles(values: List[int], ps: List[float]) -> Dict[str, Optional[int]]:
    if not values:
        return {f"p{int(p*100)}": None for p in ps}
    vals = sorted(values)
    out: Dict[str, Optional[int]] = {}
    for p in ps:
        k = int(round((len(vals) - 1) * p))
        out[f"p{int(p*100)}"] = vals[k]
    return out

def _quality_report(manifest: Dict[str, Any], extra_counts: Optional[Dict[str, int]] = None) -> Dict[str, Any]:
    chunks_path = PERSIST_DIR / "chunks.jsonl"
    lengths: List[int] = []
    by_mime: Dict[str, List[int]] = {}

    if chunks_path.exists():
        for ln in chunks_path.read_text(encoding="utf-8").splitlines():
            if not ln.strip():
                continue
            try:
                obj = json.loads(ln)
                txt = obj.get("text", "")
                lnth = len(txt)
                lengths.append(lnth)
                mime = (obj.get("meta", {}) or {}).get("mime", "unknown")
                by_mime.setdefault(mime, []).append(lnth)
            except Exception:
                pass

    avg = round(sum(lengths) / len(lengths), 1) if lengths else None
    pct = _percentiles(lengths, [0.5, 0.9, 0.99])
    longest = max(lengths) if lengths else None
    shortest = min(lengths) if lengths else None

    # MIME ë¶„í¬ + í‰ê·  ê¸¸ì´
    mime_distribution: Dict[str, Any] = {}
    for m, vs in by_mime.items():
        mime_distribution[m] = {
            "docs": sum(1 for _ in vs),  # chunk count as 'docs' proxy
            "avg_chunk_len": round(sum(vs)/len(vs), 1) if vs else None,
        }

    report = {
        "docs_total": len(manifest),
        "chunks_total": sum(m.get("chunk_count", 0) for m in manifest.values()),
        "avg_chunk_length_chars": avg,
        **pct,  # p50/p90/p99
        "longest_chunk_chars": longest,
        "shortest_chunk_chars": shortest,
        "mime_distribution": mime_distribution,
        "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
    }

    if extra_counts:
        for k in ("new_docs", "updated_docs", "unchanged_docs"):
            if k in extra_counts:
                report[k] = int(extra_counts[k])

    QUALITY_REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
    QUALITY_REPORT_PATH.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")
    return report

def _make_and_upload_backup_zip(service, backup_id: Optional[str]) -> Optional[str]:
    """
    REQ_FILESë¥¼ zipìœ¼ë¡œ ë¬¶ì–´ backup í´ë”ì— ì—…ë¡œë“œ.
    - backup_idê°€ ì—†ìœ¼ë©´ None ë°˜í™˜(ì¡°ìš©íˆ ìŠ¤í‚µ)
    - ZIPì—ëŠ” chunks.jsonl / manifest.json / quality_report.jsonì„ í¬í•¨
    """
    if not backup_id:
        return None

    ts = time.strftime("%Y%m%d-%H%M%S")
    zip_path = APP_DATA_DIR / f"index_backup_{ts}.zip"

    # ZIP ì‘ì„± (ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í¬í•¨)
    with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        cj = PERSIST_DIR / "chunks.jsonl"
        if cj.exists():
            zf.write(cj, arcname="chunks.jsonl")
        if MANIFEST_PATH.exists():
            zf.write(MANIFEST_PATH, arcname="manifest.json")
        if QUALITY_REPORT_PATH.exists():
            zf.write(QUALITY_REPORT_PATH, arcname="quality_report.json")

    # ì—…ë¡œë“œ â†’ ì„ì‹œ ZIP ì‚­ì œ
    try:
        _id = _upload_zip(service, backup_id, zip_path, zip_path.name)
        return _id
    finally:
        try:
            zip_path.unlink(missing_ok=True)
        except Exception:
            pass




# ===== [09] PUBLIC ENTRY (APPì—ì„œ í˜¸ì¶œ) ======================================
def build_index_with_checkpoint(
    update_pct: Callable[[int, str | None], None],
    update_msg: Callable[[str], None],
    gdrive_folder_id: str,                 # (í˜¸í™˜ ìœ„í•´ ìœ ì§€: prepared IDë¡œ ì‚¬ìš© ê°€ëŠ¥)
    gcp_creds: Mapping[str, object],
    persist_dir: str,                      # (í˜¸í™˜, í˜„ì¬ ë‚´ë¶€ PERSIST_DIR ì‚¬ìš©)
    remote_manifest: Dict[str, Dict[str, object]],
    should_stop: Callable[[], bool] | None = None,
) -> Dict[str, Any]]:
    """
    ëª©í‘œ:
      - ì•± ì²« ì‹¤í–‰ ì‹œ: prepared í´ë” ë‚´ìš©ì„ ìµœì í™”í•˜ì—¬ ë¡œì»¬ ì¸ë±ìŠ¤ ìƒì„±
      - ì´í›„ preparedì— ìƒˆ íŒŒì¼ì´ ì¶”ê°€ë˜ë©´: ë¸íƒ€ë§Œ ëˆ„ì  ë°˜ì˜
      - í•­ìƒ ì™„ë£Œ í›„ backup_zip í´ë”ì— zip ì—…ë¡œë“œ(ìˆìœ¼ë©´)
    ë°˜í™˜: ì²˜ë¦¬ ìš”ì•½(dict)
    """
    # ì•ˆì „ ì½œë°±
    def _pct(v: int, msg: str | None = None):
        try:
            update_pct(int(v), msg)
        except Exception:
            pass
    def _msg(s: str):
        try:
            update_msg(str(s))
        except Exception:
            pass

    _msg("ğŸ” Preparing Google Drive client (OAuth first)â€¦")
    svc = _drive_client()
    _pct(5, "drive-ready")

    # í´ë” ID ê²°ì •
    prepared_id = _find_folder_id("PREPARED", fallback=gdrive_folder_id)
    backup_id   = _find_folder_id("BACKUP") or _find_folder_id("DEFAULT")
    if not prepared_id:
        raise KeyError("prepared í´ë” IDë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (GDRIVE_PREPARED_FOLDER_ID / PREPARED_FOLDER_ID / gdrive_folder_id ì¸ì)")

    # ìš”êµ¬ì‚¬í•­: ë°±ì—… ZIPì´ ìˆì–´ë„ í•­ìƒ prepared ê¸°ì¤€ ë¸íƒ€ ë¹Œë“œë¡œ ëˆ„ì  ë°˜ì˜
    _msg("ğŸ“¦ Scanning prepared folder and building deltaâ€¦")
    processed, chunks, manifest, stats = _build_from_prepared(svc, prepared_id)
    _pct(70, f"processed={processed}, chunks={chunks}")

    if should_stop and should_stop():
        return {"ok": False, "stopped": True}

    _msg("ğŸ§® Writing quality reportâ€¦")
    report = _quality_report(manifest, extra_counts=stats)
    _pct(85, "report-ready")

    _msg("â¬†ï¸ Uploading backup zipâ€¦")
    uploaded_id = _make_and_upload_backup_zip(svc, backup_id)
    _pct(100, "done")

    return {
        "ok": True,
        "processed_files": processed,
        "generated_chunks": chunks,
        "stats": stats,
        "report": report,
        "backup_zip_id": uploaded_id,
        "prepared_folder_id": prepared_id,
        "backup_folder_id": backup_id,
        "auth_mode": "oauth-first"  # ë””ë²„ê·¸ìš©
    }


# ===== [10] (ì„ íƒ) CLI í…ŒìŠ¤íŠ¸ìš© ì—”íŠ¸ë¦¬ =======================================
if __name__ == "__main__":
    def _noop_pct(v: int, msg: str | None = None): ...
    def _noop_msg(s: str): ...
    res = build_index_with_checkpoint(_noop_pct, _noop_msg, gdrive_folder_id="", gcp_creds={}, persist_dir="", remote_manifest={})
    print(json.dumps(res, ensure_ascii=False, indent=2))
# =============================================================================
