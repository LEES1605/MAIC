# ===== [01] IMPORTS & UTILS FALLBACK ========================================  # [01] START
from __future__ import annotations

import importlib
import io
import os
import shutil
import tempfile
import zipfile
from pathlib import Path
from types import ModuleType
from typing import Any, Dict, Optional, Protocol, Mapping, cast

import requests

# streamlitì€ ìˆì„ ìˆ˜ë„/ì—†ì„ ìˆ˜ë„ ìˆë‹¤.
# - mypy ì¶©ëŒ ë°©ì§€: st ë³€ìˆ˜ë¥¼ ë¨¼ì € Any|Noneë¡œ ì„ ì–¸ í›„, ëŸ°íƒ€ì„ì— ëª¨ë“ˆ/Noneì„ ëŒ€ì…
from typing import Any as _AnyForSt
st: _AnyForSt | None
try:
    import streamlit as _st_mod
    st = cast(_AnyForSt, _st_mod)
except Exception:
    st = None  # mypy OK: Optional[Any]

# ê³µìš© ìœ í‹¸: ëª¨ë“ˆ ë™ì  ì„í¬íŠ¸ í›„ ì¡´ì¬ í™•ì¸ + í´ë°± ì œê³µ
_utils: ModuleType | None
try:
    _utils = importlib.import_module("src.common.utils")
except Exception:
    _utils = None  # ëª¨ë“ˆ ìì²´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ


# --- ì •ì  ì¸í„°í˜ì´ìŠ¤(Protocol) ------------------------------------------------
class _LoggerProto(Protocol):
    def info(self, *a: Any, **k: Any) -> None: ...
    def warning(self, *a: Any, **k: Any) -> None: ...
    def error(self, *a: Any, **k: Any) -> None: ...


def get_secret(name: str, default: str = "") -> str:
    """Streamlit secrets â†’ env â†’ default ìˆœìœ¼ë¡œ ì¡°íšŒ(ë°˜í™˜ì€ í•­ìƒ str)."""
    # 1) src.common.utils.get_secret ìš°ì„ 
    if _utils is not None:
        func = getattr(_utils, "get_secret", None)
        if callable(func):
            try:
                val = func(name, default)
                # ì™¸ë¶€ utilì´ Optional/ë¹„ë¬¸ìì—´ì„ ì¤„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ ë³€í™˜
                if val is None:
                    return default
                return val if isinstance(val, str) else str(val)
            except Exception:
                pass

    # 2) streamlit.secrets (ì •ì  íƒ€ì… ê°€ë“œ + ë§¤í•‘ ìºìŠ¤íŒ…)
    try:
        if st is not None and hasattr(st, "secrets"):
            sec = cast(Mapping[str, Any], st.secrets)  # runtimeì€ Mapping ìœ ì‚¬ì²´
            v = sec.get(name, None)
            if v is not None:
                return v if isinstance(v, str) else str(v)
    except Exception:
        pass

    # 3) í™˜ê²½ë³€ìˆ˜ (Optional â†’ strë¡œ ê°•ì œ)
    env_v = os.getenv(name)
    return env_v if env_v is not None else default


def logger() -> _LoggerProto:
    """src.common.utils.logger()ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ _Logger(Protocol ì¤€ìˆ˜) ë°˜í™˜."""
    if _utils is not None:
        func = getattr(_utils, "logger", None)
        if callable(func):
            try:
                lg = func()
                # ì™¸ë¶€ êµ¬í˜„ì´ ë¬´ì—‡ì´ë“ , ìµœì†Œí•œ Protocol ì¶©ì¡± ë³´ì¥(duck typing)
                return cast(_LoggerProto, lg)
            except Exception:
                pass

    class _Logger:
        def info(self, *a: Any, **k: Any) -> None: ...
        def warning(self, *a: Any, **k: Any) -> None: ...
        def error(self, *a: Any, **k: Any) -> None: ...

    return _Logger()
# [01] END =====================================================================


# ===== [02] CONSTANTS & PUBLIC EXPORTS =======================================  # [02] START
API = "https://api.github.com"
__all__ = ["restore_latest", "get_latest_release"]
# [02] END =====================================================================


# ===== [03] HEADERS / LOG HELPERS ============================================  # [03] START
def _repo() -> str:
    """ëŒ€ìƒ ì €ì¥ì†Œ 'owner/repo' ë¬¸ìì—´ì„ ì¡°íšŒ."""
    return get_secret("GITHUB_REPO", "") or os.getenv("GITHUB_REPO", "")


def _headers(binary: bool = False) -> Dict[str, str]:
    """GitHub API í˜¸ì¶œìš© ê¸°ë³¸ í—¤ë” êµ¬ì„±."""
    token = get_secret("GITHUB_TOKEN", "") or os.getenv("GITHUB_TOKEN", "")
    h: Dict[str, str] = {
        "Accept": "application/vnd.github+json",
        "User-Agent": "maic-backup",
    }
    if token:
        h["Authorization"] = f"token {token}"
    if binary:
        h["Accept"] = "application/octet-stream"
    return h


def _log(msg: str) -> None:
    """ê°€ëŠ¥í•˜ë©´ logger/streamlitë¡œë„ ë©”ì‹œì§€ë¥¼ ì¶œë ¥."""
    try:
        logger().info(msg)
    except Exception:
        pass
    if st is not None:
        try:
            st.write(msg)
        except Exception:
            pass
# [03] END =====================================================================


# ===== [04] RELEASE DISCOVERY =================================================  # [04] START
def _latest_release(repo: str) -> Optional[dict]:
    """ê°€ì¥ ìµœì‹  ë¦´ë¦¬ìŠ¤ë¥¼ ì¡°íšŒ. ì‹¤íŒ¨ ì‹œ None."""
    if not repo:
        _log("GITHUB_REPOê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    url = f"{API}/repos/{repo}/releases/latest"
    try:
        r = requests.get(url, headers=_headers(), timeout=15)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        _log(f"ìµœì‹  ë¦´ë¦¬ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return None


def _pick_best_asset(rel: dict) -> Optional[dict]:
    """ë¦´ë¦¬ìŠ¤ ìì‚° ì¤‘ ìš°ì„ ìˆœìœ„(.zip > .tar.gz/.tgz > .gz > ì²« ë²ˆì§¸)ë¥¼ ì„ íƒ."""
    assets = rel.get("assets") or []
    if not assets:
        return None
    # 1) zip
    for a in assets:
        if str(a.get("name", "")).lower().endswith(".zip"):
            return a
    # 2) tar.gz / tgz
    for a in assets:
        n = str(a.get("name", "")).lower()
        if n.endswith(".tar.gz") or n.endswith(".tgz"):
            return a
    # 3) ë‹¨ì¼ gz (ì˜ˆ: chunks.jsonl.gz)
    for a in assets:
        n = str(a.get("name", "")).lower()
        if n.endswith(".gz"):
            return a
    # 4) ê·¸ ì™¸ ì²« ë²ˆì§¸
    return assets[0] if assets else None
# [04] END =====================================================================

# ===== [05] ASSET DOWNLOAD & EXTRACT =========================================  # [05] START
def _download_asset(asset: dict) -> Optional[bytes]:
    """GitHub ë¦´ë¦¬ìŠ¤ ìì‚°ì„ ë‚´ë ¤ë°›ì•„ ë°”ì´íŠ¸ë¡œ ë°˜í™˜. ì‹¤íŒ¨ ì‹œ None."""
    url = asset.get("url") or asset.get("browser_download_url")
    if not url:
        return None
    try:
        r = requests.get(url, headers=_headers(binary=True), timeout=60)
        r.raise_for_status()
        return r.content
    except Exception as e:
        _log(f"ìì‚° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return None


def _extract_zip(data: bytes, dest_dir: Path) -> bool:
    """ZIP ë°”ì´íŠ¸ë¥¼ dest_dirì— í’€ê¸°. ì„±ê³µ True/ì‹¤íŒ¨ False."""
    try:
        with zipfile.ZipFile(io.BytesIO(data)) as zf:
            zf.extractall(dest_dir)
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(zip): {type(e).__name__}: {e}")
        return False


def _extract_targz(data: bytes, dest_dir: Path) -> bool:
    """TAR.GZ / TGZ ë°”ì´íŠ¸ë¥¼ dest_dirì— í’€ê¸°."""
    try:
        import tarfile
        with tarfile.open(fileobj=io.BytesIO(data), mode="r:gz") as tf:
            tf.extractall(dest_dir)
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(tar.gz): {type(e).__name__}: {e}")
        return False


def _extract_gz_to_file(asset_name: str, data: bytes, dest_dir: Path) -> bool:
    """ë‹¨ì¼ .gz(ì˜ˆ: chunks.jsonl.gz)ë¥¼ dest_dir/<basename>ìœ¼ë¡œ í’€ê¸°."""
    try:
        import gzip  # ì§€ì—­ ì„í¬íŠ¸ë¡œ ìƒë‹¨ êµ¬íš ë³€ê²½ ë¶ˆí•„ìš”
        base = asset_name[:-3] if asset_name.lower().endswith(".gz") else asset_name
        out_path = dest_dir / base
        with gzip.GzipFile(fileobj=io.BytesIO(data), mode="rb") as gf:
            out_path.write_bytes(gf.read())
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(gz): {type(e).__name__}: {e}")
        return False


def _extract_auto(asset_name: str, data: bytes, dest_dir: Path) -> bool:
    """ìì‚° ì´ë¦„ìœ¼ë¡œ í˜•ì‹ì„ ìœ ì¶”í•˜ì—¬ ì ì ˆíˆ í•´ì œ."""
    n = (asset_name or "").lower()
    if n.endswith(".zip"):
        return _extract_zip(data, dest_dir)
    if n.endswith(".tar.gz") or n.endswith(".tgz"):
        return _extract_targz(data, dest_dir)
    if n.endswith(".gz"):
        return _extract_gz_to_file(asset_name, data, dest_dir)
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹: zip ì‹œë„(ì‹¤íŒ¨ ì‹œ False)
    return _extract_zip(data, dest_dir)
# [05] END =====================================================================

# ===== [06] PUBLIC API: restore_latest =======================================  # [06] START
def restore_latest(dest_dir: str | Path) -> bool:
    """ìµœì‹  GitHub Releaseì—ì„œ ì•„í‹°íŒ©íŠ¸ë¥¼ ë‚´ë ¤ë°›ì•„ dest_dirì— ë³µì›.

    ë°˜í™˜:
        ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False (ì˜ˆì™¸ëŠ” ì˜¬ë¦¬ì§€ ì•ŠìŒ)

    ë¹„ê³ :
        - .zip/.tar.gz/.tgz/.gz ëª¨ë‘ ì²˜ë¦¬
        - ì••ì¶• í•´ì œ ê²°ê³¼ê°€ 'ìµœìƒìœ„ ë‹¨ì¼ í´ë”'ì¼ ê²½ìš°, ê·¸ í´ë”ë¥¼ í•œ ê²¹ í‰íƒ„í™”í•˜ì—¬
          í´ë” ë‚´ë¶€ì˜ íŒŒì¼/ë””ë ‰í„°ë¦¬ë¥¼ dest_dir ë°”ë¡œ ì•„ë˜ë¡œ ë³µì‚¬í•œë‹¤.
        - ì´í›„ dest ë‚´ ì‚°ì¶œë¬¼ì„ ì •ë¦¬í•˜ì—¬ chunks.jsonlì„ ë£¨íŠ¸ë¡œ ëª¨ìœ¼ê³  .readyë¥¼ ë³´ì •í•œë‹¤.
    """
    dest = Path(dest_dir).expanduser()
    dest.mkdir(parents=True, exist_ok=True)

    repo = _repo()
    if not repo:
        _log("restore_latest: GITHUB_REPO ë¯¸ì„¤ì •")
        return False

    rel = _latest_release(repo)
    if not rel:
        return False

    name = rel.get("name") or rel.get("tag_name") or "(no-tag)"
    _log(f"ìµœì‹  ë¦´ë¦¬ìŠ¤: {name}")

    asset = _pick_best_asset(rel)
    if not asset:
        _log("ë¦´ë¦¬ìŠ¤ì— ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ìì‚°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    asset_name = str(asset.get("name") or "")
    _log(f"ìì‚° ë‹¤ìš´ë¡œë“œ: {asset_name}")
    data = _download_asset(asset)
    if not data:
        return False

    # ì„ì‹œ ë””ë ‰í„°ë¦¬ë¥¼ ì‚¬ìš©í•´ ì›ìì  êµì²´ì— ê°€ê¹ê²Œ ë³µì›
    with tempfile.TemporaryDirectory() as td:
        tmp = Path(td)

        # 1) ì••ì¶• í•´ì œ (.zip/.tar.gz/.tgz/.gz ìë™ íŒë³„)
        ok = _extract_auto(asset_name, data, tmp)
        if not ok:
            return False

        # 2) 'ìµœìƒìœ„ ë‹¨ì¼ í´ë”' ê°ì§€ â†’ í‰íƒ„í™” ëŒ€ìƒ ë£¨íŠ¸ ê²°ì •
        children = [
            p
            for p in tmp.iterdir()
            if p.name not in (".DS_Store",) and not p.name.startswith("__MACOSX")
        ]
        src_root = tmp
        if len(children) == 1 and children[0].is_dir():
            src_root = children[0]
            _log("í‰íƒ„í™” ì ìš©: ìµœìƒìœ„ í´ë” ë‚´ë¶€ë¥¼ ë£¨íŠ¸ë¡œ ìŠ¹ê²©")
            _log(f"ìŠ¹ê²© ëŒ€ìƒ í´ë”: '{src_root.name}'")

        # 3) ë³µì‚¬(ê¸°ì¡´ ë™ì¼ ê²½ë¡œëŠ” êµì²´). 'í´ë” ìì²´'ê°€ ì•„ë‹ˆë¼ 'í´ë” ë‚´ë¶€'ë¥¼ ë³µì‚¬í•œë‹¤.
        for p in src_root.iterdir():
            target = dest / p.name
            try:
                if target.exists():
                    if target.is_dir():
                        shutil.rmtree(target)
                    else:
                        target.unlink()
                if p.is_dir():
                    shutil.copytree(p, target)
                else:
                    shutil.copy2(p, target)
            except Exception as e:
                _log("íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨(ì¼ë¶€ í•­ëª©). ë‹¤ìŒ ë¼ì¸ì— ìƒì„¸ í‘œì‹œ.")
                _log(f"ì›ë³¸: {p.name} â†’ ëŒ€ìƒ: {target.name} â€” {type(e).__name__}: {e}")
                return False

    # 4) ì‚°ì¶œë¬¼ ì •ë¦¬(ê°•í™”): dest ì•ˆì—ì„œ chunks.jsonlì„ ë£¨íŠ¸ë¡œ ëª¨ìœ¼ê¸°
    def _size(p: Path) -> int:
        try:
            return p.stat().st_size
        except Exception:
            return 0

    def _decompress_gz(src: Path, dst: Path) -> bool:
        try:
            import gzip
            with gzip.open(src, "rb") as gf:
                data2 = gf.read()
            if not data2:
                return False
            dst.write_bytes(data2)
            return True
        except Exception as e:
            _log(f"gz í•´ì œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return False

    def _merge_dir_jsonl(chunk_dir: Path, out_file: Path) -> bool:
        """chunk_dir ì•ˆì˜ *.jsonlì„ ë¼ì¸ ë³´ì¡´ìœ¼ë¡œ ë³‘í•©í•œë‹¤."""
        try:
            bytes_written = 0
            tmp_out = out_file.with_suffix(".jsonl.tmp")
            if tmp_out.exists():
                tmp_out.unlink()
            with tmp_out.open("wb") as w:
                for p in sorted(chunk_dir.glob("*.jsonl")):
                    try:
                        with p.open("rb") as r:
                            while True:
                                buf = r.read(1024 * 1024)
                                if not buf:
                                    break
                                w.write(buf)
                                bytes_written += len(buf)
                    except Exception:
                        continue
            if bytes_written > 0:
                if out_file.exists():
                    out_file.unlink()
                tmp_out.replace(out_file)
                return True
            tmp_out.unlink(missing_ok=True)
            return False
        except Exception as e:
            _log(f"chunks/ ë³‘í•© ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return False

    target = dest / "chunks.jsonl"

    def _consolidate_to_target(root: Path, target_file: Path) -> bool:
        # ì´ë¯¸ ìœ íš¨í•˜ë©´ ë
        if target_file.exists() and _size(target_file) > 0:
            return True

        # a) ì •í™•ëª… ìš°ì„ : chunks.jsonl / chunks.jsonl.gz (ì„ì˜ ê¹Šì´)
        try:
            exact = [p for p in root.rglob("chunks.jsonl") if _size(p) > 0]
        except Exception:
            exact = []
        if exact:
            best = max(exact, key=_size)
            shutil.copy2(best, target_file)
            _log(f"exact chunks.jsonl ì‚¬ìš©: {best}")
            return True

        try:
            exact_gz = [p for p in root.rglob("chunks.jsonl.gz") if _size(p) > 0]
        except Exception:
            exact_gz = []
        if exact_gz:
            best_gz = max(exact_gz, key=_size)
            if _decompress_gz(best_gz, target_file):
                _log(f"exact chunks.jsonl.gz í•´ì œ: {best_gz}")
                return True

        # b) ë””ë ‰í„°ë¦¬ ë³‘í•©: */chunks/*.jsonl
        try:
            chunk_dirs = [d for d in root.rglob("chunks") if d.is_dir()]
        except Exception:
            chunk_dirs = []
        for d in chunk_dirs:
            if _merge_dir_jsonl(d, target_file):
                _log(f"ë””ë ‰í„°ë¦¬ ë³‘í•© ì‚¬ìš©: {d}")
                return True

        # c) ë²”ìš© íŒŒì¼: ì„ì˜ì˜ *.jsonl / *.jsonl.gz ì¤‘ ê°€ì¥ í° ê²ƒ ì„ íƒ
        try:
            any_jsonl = [p for p in root.rglob("*.jsonl") if _size(p) > 0]
        except Exception:
            any_jsonl = []
        if any_jsonl:
            best_any = max(any_jsonl, key=_size)
            shutil.copy2(best_any, target_file)
            _log(f"ì„ì˜ *.jsonl ì‚¬ìš©: {best_any}")
            return True

        try:
            any_gz = [p for p in root.rglob("*.jsonl.gz") if _size(p) > 0]
        except Exception:
            any_gz = []
        if any_gz:
            best_any_gz = max(any_gz, key=_size)
            if _decompress_gz(best_any_gz, target_file):
                _log(f"ì„ì˜ *.jsonl.gz í•´ì œ: {best_any_gz}")
                return True

        # ì‹¤íŒ¨ ì‹œ 0ë°”ì´íŠ¸ targetì´ ìˆìœ¼ë©´ ì œê±°
        if target_file.exists() and _size(target_file) == 0:
            target_file.unlink(missing_ok=True)
        return False

    ok_cons = _consolidate_to_target(dest, target)

    # ğŸ” ìµœì¢… í´ë°±: ë¦´ë¦¬ìŠ¤ ìì‚°ì´ chunks.jsonl.gz ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°, ì›ë³¸ ë°”ì´íŠ¸ë¡œ ì§ì ‘ í•´ì œ
    if not ok_cons and asset_name.lower().endswith(".gz") and data:
        try:
            import gzip
            raw = gzip.decompress(data)
            if raw:
                target.parent.mkdir(parents=True, exist_ok=True)
                target.write_bytes(raw)
                _log("ìì‚° ë°”ì´íŠ¸ ì§ì ‘ í•´ì œ: chunks.jsonl ìƒì„±(í´ë°±)")
                ok_cons = True
        except Exception as e:
            _log(f"í´ë°± í•´ì œ ì‹¤íŒ¨: {type(e).__name__}: {e}")

    if not ok_cons:
        _log("ì‚°ì¶œë¬¼ ì •ë¦¬ ì‹¤íŒ¨: chunks.jsonlì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # READY ë³´ì • ì—†ì´ ì¢…ë£Œ
        return False

    # 5) SSOT ë³´ì •: chunks.jsonlë§Œ ì¡´ì¬í•˜ê³  .readyê°€ ì—†ìœ¼ë©´ ìƒì„±
    try:
        chunks = dest / "chunks.jsonl"
        ready = dest / ".ready"
        if chunks.exists() and chunks.stat().st_size > 0 and not ready.exists():
            ready.write_text("ok", encoding="utf-8")
    except Exception:
        pass

    _log("ë³µì›ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return True
# ===== [06] PUBLIC API: restore_latest =======================================  # [06] END


# ===== [07] PUBLIC API: publish_backup =======================================  # [07] START
def publish_backup(persist_dir: str | Path, tag_prefix: str = "index") -> dict | None:
    """
    í˜„ì¬ ë¡œì»¬ ì¸ë±ìŠ¤(PERSIST_DIR/chunks.jsonl)ë¥¼ GitHub Releaseë¡œ ë°±ì—… ë°œí–‰.
    - ìƒˆ íƒœê·¸: {tag_prefix}-YYYYMMDD-HHMMSS
    - ìì‚°: chunks.jsonl.gz
    ë°˜í™˜: {"tag": "...", "release_id": int, "asset": "chunks.jsonl.gz", "size": int} ë˜ëŠ” None
    """
    from pathlib import Path
    import os
    import io
    import gzip
    import json
    import datetime
    import requests  # type: ignore

    dest = Path(persist_dir).expanduser()
    src = dest / "chunks.jsonl"
    if not (src.exists() and src.stat().st_size > 0):
        _log("publish_backup: chunks.jsonlì´ ì—†ê±°ë‚˜ 0B")
        return None

    repo = _repo()
    if not repo:
        _log("publish_backup: GITHUB_REPO ë¯¸ì„¤ì •")
        return None

    token = None
    try:
        token = GITHUB_TOKEN  # noqa: F821  (ëª¨ë“ˆ ìƒë‹¨ì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
    except Exception:
        token = None
    if not token:
        token = os.getenv("GITHUB_TOKEN", "")

    if not token:
        _log("publish_backup: GITHUB_TOKEN ë¯¸ì„¤ì •")
        return None

    # ìì‚° gzip
    gz_name = "chunks.jsonl.gz"
    buf = io.BytesIO()
    with gzip.GzipFile(filename="chunks.jsonl", mode="wb", fileobj=buf) as z:
        z.write(src.read_bytes())
    data_bytes = buf.getvalue()

    # ë¦´ë¦¬ìŠ¤ ìƒì„±
    now = datetime.datetime.utcnow().strftime("%Y%m%d-%H%M%S")
    tag = f"{tag_prefix}-{now}"
    api = f"https://api.github.com/repos/{repo}/releases"
    headers = {"Authorization": f"token {token}", "Accept": "application/vnd.github+json"}

    payload = {"tag_name": tag, "name": tag, "draft": False, "prerelease": False}
    try:
        res = requests.post(api, headers=headers, data=json.dumps(payload), timeout=30)
        if res.status_code >= 300:
            _log(f"publish_backup: ë¦´ë¦¬ìŠ¤ ìƒì„± ì‹¤íŒ¨ {res.status_code} {res.text[:200]}")
            return None
        rel = res.json()
        upload_url = rel.get("upload_url", "")
        # upload_url ì˜ˆ: https://uploads.github.com/repos/{repo}/releases/{id}/assets{?name,label}
        upload_url = upload_url.split("{", 1)[0] + f"?name={gz_name}"
        rid = int(rel.get("id") or 0)
    except Exception as e:
        _log(f"publish_backup: ìš”ì²­ ì‹¤íŒ¨ â€” {type(e).__name__}: {e}")
        return None

    # ìì‚° ì—…ë¡œë“œ
    try:
        up_headers = {
            "Authorization": f"token {token}",
            "Content-Type": "application/gzip",
            "Accept": "application/vnd.github+json",
        }
        res2 = requests.post(upload_url, headers=up_headers, data=data_bytes, timeout=60)
        if res2.status_code >= 300:
            _log(f"publish_backup: ìì‚° ì—…ë¡œë“œ ì‹¤íŒ¨ {res2.status_code} {res2.text[:200]}")
            return None
    except Exception as e:
        _log(f"publish_backup: ì—…ë¡œë“œ ì˜ˆì™¸ â€” {type(e).__name__}: {e}")
        return None

    _log(f"ë°±ì—… ë°œí–‰ ì™„ë£Œ: {tag} ({len(data_bytes)}B)")
    return {"tag": tag, "release_id": rid, "asset": gz_name, "size": len(data_bytes)}
# ===== [07] PUBLIC API: publish_backup =======================================  # [07] END
