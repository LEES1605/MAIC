# ===== [01] COMMON HELPERS =====================================================  # [01] START
from __future__ import annotations

import os
import importlib
from typing import Any, Dict
from pathlib import Path  # E402 ë°©ì§€: Pathë¥¼ ìµœìƒë‹¨ì—ì„œ ì„í¬íŠ¸

API = "https://api.github.com"


def _log(msg: str) -> None:
    """í”„ë¡œì íŠ¸ ì–´ë””ì„œ í˜¸ì¶œí•´ë„ ì•ˆì „í•œ ì´ˆê°„ë‹¨ ë¡œê±°."""
    try:
        # ë™ì  ì„í¬íŠ¸ë¡œ mypy attr-defined íšŒí”¼
        mod = importlib.import_module("src.state.session")
        fn = getattr(mod, "append_admin_log", None)
        if callable(fn):
            fn(str(msg))
            return
    except Exception:
        pass
    try:
        import logging
        logging.getLogger("maic.backup").info(str(msg))
    except Exception:
        # ìµœí›„ ìˆ˜ë‹¨
        print(str(msg))


def _get_env(name: str, default: str = "") -> str:
    v = os.getenv(name)
    return v if isinstance(v, str) and v.strip() else default


def _token() -> str:
    t = _get_env("GITHUB_TOKEN")
    if t:
        return t
    # ë™ì  ì„í¬íŠ¸ë¡œ mypy import-not-found íšŒí”¼
    try:
        mod = importlib.import_module("src.backup.github_config")
        tk = getattr(mod, "GITHUB_TOKEN", "")
        return str(tk) if tk else ""
    except Exception:
        return ""


def _repo() -> str:
    r = _get_env("GITHUB_REPO")
    if r:
        return r
    try:
        mod = importlib.import_module("src.backup.github_config")
        rp = getattr(mod, "GITHUB_REPO", "")
        return str(rp) if rp else ""
    except Exception:
        return ""


def _branch() -> str:
    b = _get_env("GITHUB_BRANCH", "main")
    if b:
        return b
    try:
        mod = importlib.import_module("src.backup.github_config")
        br = getattr(mod, "GITHUB_BRANCH", "main")
        return str(br or "main")
    except Exception:
        return "main"


def _headers() -> Dict[str, str]:
    """GitHub API ê³µí†µ í—¤ë”."""
    t = _token()
    h: Dict[str, str] = {
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
        "User-Agent": "maic-backup-bot",
    }
    if t:
        h["Authorization"] = f"Bearer {t}"
    return h


def _upload_headers(content_type: str) -> Dict[str, str]:
    h = _headers()
    h["Content-Type"] = content_type
    return h
# ===== [01] COMMON HELPERS =====================================================  # [01] END


# ===== [02] CONSTANTS & PUBLIC EXPORTS =======================================  # [02] START
__all__ = ["restore_latest", "get_latest_release", "publish_backup"]
# [02] END =====================================================================


# ===== [03] LEGACY PUBLISH PLACEHOLDER ========================================  # [03] START
"""
[DEPRECATED]
ì´ êµ¬íšì˜ publish_backup êµ¬í˜„ì€ íê¸°ë˜ì—ˆìŠµë‹ˆë‹¤.
ì‹¤ì œ êµ¬í˜„ì€ [07] êµ¬íšì˜ `publish_backup`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
ë³¸ ì„¹ì…˜ì€ ì¤‘ë³µ ì •ì˜(F811)ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë”ì…ë‹ˆë‹¤.
"""
# (í•¨ìˆ˜ ì •ì˜ ì—†ìŒ)
# ===== [03] LEGACY PUBLISH PLACEHOLDER ========================================  # [03] END




# ===== [04] RELEASE DISCOVERY =================================================  # [04] START
def _latest_release(repo: str) -> dict | None:
    """ê°€ì¥ ìµœì‹  ë¦´ë¦¬ìŠ¤ë¥¼ ì¡°íšŒ. ì‹¤íŒ¨ ì‹œ None."""
    if not repo:
        _log("GITHUB_REPOê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    import requests  # E402 íšŒí”¼: í•¨ìˆ˜ ë‚´ë¶€ ë¡œì»¬ ì„í¬íŠ¸
    url = f"{API}/repos/{repo}/releases/latest"
    try:
        r = requests.get(url, headers=_headers(), timeout=15)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        _log(f"ìµœì‹  ë¦´ë¦¬ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return None


def get_latest_release(repo: str | None = None) -> dict | None:
    """
    PUBLIC API: ìµœì‹  GitHub ë¦´ë¦¬ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    - repo ì¸ìê°€ ì—†ìœ¼ë©´ secrets/envì˜ GITHUB_REPOë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ìš”ì²­/íŒŒì‹± ì‹¤íŒ¨ ì‹œ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤(ì˜ˆì™¸ ë°œìƒí•˜ì§€ ì•ŠìŒ).
    """
    target = (repo or _repo()).strip()
    rel = _latest_release(target)
    if rel is None:
        return None
    # ìµœì†Œ í•„ë“œ ì •ê·œí™”(í˜¸ì¶œì¸¡ í¸ì˜)
    if "tag_name" not in rel and "name" in rel:
        rel["tag_name"] = rel.get("name")
    return rel


def _pick_best_asset(rel: dict) -> dict | None:
    """ë¦´ë¦¬ìŠ¤ ìì‚° ì¤‘ ìš°ì„ ìˆœìœ„(.zip > .tar.gz > .gz > ì²« ë²ˆì§¸)ë¥¼ ì„ íƒ."""
    assets = rel.get("assets") or []
    if not assets:
        return None
    for a in assets:
        if str(a.get("name", "")).lower().endswith(".zip"):
            return a
    for a in assets:
        if str(a.get("name", "")).lower().endswith(".tar.gz"):
            return a
    for a in assets:
        if str(a.get("name", "")).lower().endswith(".gz"):
            return a
    return assets[0] if assets else None
# [04] END



# ===== [05] ASSET DOWNLOAD & EXTRACT =========================================  # [05] START
def _download_asset(asset: dict) -> bytes | None:
    """GitHub ë¦´ë¦¬ìŠ¤ ìì‚°ì„ ë‚´ë ¤ë°›ì•„ ë°”ì´íŠ¸ë¡œ ë°˜í™˜. ì‹¤íŒ¨ ì‹œ None."""
    url = asset.get("url") or asset.get("browser_download_url")
    if not url:
        return None
    try:
        import requests  # E402 íšŒí”¼: í•¨ìˆ˜ ë‚´ë¶€ ë¡œì»¬ ì„í¬íŠ¸
        # GitHub 'assets/:id' APIëŠ” application/octet-streamì„ ìš”êµ¬
        hdrs = dict(_headers())
        if "releases/assets/" in url and "browser_download_url" not in asset:
            hdrs["Accept"] = "application/octet-stream"
        r = requests.get(url, headers=hdrs, timeout=60)
        r.raise_for_status()
        return r.content
    except Exception as e:
        _log(f"ìì‚° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return None


def _extract_zip(data: bytes, dest_dir: Path) -> bool:
    """ZIP ë°”ì´íŠ¸ë¥¼ dest_dirì— í’€ê¸°. ì„±ê³µ True/ì‹¤íŒ¨ False."""
    try:
        import io, zipfile  # E402 íšŒí”¼: í•¨ìˆ˜ ë‚´ë¶€ ë¡œì»¬ ì„í¬íŠ¸
        with zipfile.ZipFile(io.BytesIO(data)) as zf:
            zf.extractall(dest_dir)
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(zip): {type(e).__name__}: {e}")
        return False


def _extract_targz(data: bytes, dest_dir: Path) -> bool:
    """TAR.GZ / TGZ ë°”ì´íŠ¸ë¥¼ dest_dirì— í’€ê¸°."""
    try:
        import tarfile, io  # E402 íšŒí”¼
        with tarfile.open(fileobj=io.BytesIO(data), mode="r:gz") as tf:
            tf.extractall(dest_dir)
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(tar.gz): {type(e).__name__}: {e}")
        return False


def _extract_gz_to_file(asset_name: str, data: bytes, dest_dir: Path) -> bool:
    """ë‹¨ì¼ .gz(ì˜ˆ: chunks.jsonl.gz)ë¥¼ dest_dir/<basename>ìœ¼ë¡œ í’€ê¸°."""
    try:
        import gzip, io  # E402 íšŒí”¼
        base = asset_name[:-3] if asset_name.lower().endswith(".gz") else asset_name
        out_path = dest_dir / base
        with gzip.GzipFile(fileobj=io.BytesIO(data), mode="rb") as gf:
            out_path.write_bytes(gf.read())
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(gz): {type(e).__name__}: {e}")
        return False


def _extract_auto(asset_name: str, data: bytes, dest_dir: Path) -> bool:
    """ìì‚° ì´ë¦„ìœ¼ë¡œ í˜•ì‹ì„ ìœ ì¶”í•˜ì—¬ ì ì ˆíˆ í•´ì œ."""
    n = (asset_name or "").lower()
    if n.endswith(".zip"):
        return _extract_zip(data, dest_dir)
    if n.endswith(".tar.gz") or n.endswith(".tgz"):
        return _extract_targz(data, dest_dir)
    if n.endswith(".gz"):
        return _extract_gz_to_file(asset_name, data, dest_dir)
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹: zip ì‹œë„(ì‹¤íŒ¨ ì‹œ False)
    return _extract_zip(data, dest_dir)
# [05] END =====================================================================


# ===== [06] PUBLIC API: restore_latest =======================================  # [06] START
def restore_latest(dest_dir: str | Path) -> bool:
    """ìµœì‹  GitHub Releaseì—ì„œ ì•„í‹°íŒ©íŠ¸ë¥¼ ë‚´ë ¤ë°›ì•„ dest_dirì— ë³µì›.

    ë°˜í™˜:
        ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False (ì˜ˆì™¸ëŠ” ì˜¬ë¦¬ì§€ ì•ŠìŒ)

    ë¹„ê³ :
        - .zip/.tar.gz/.tgz/.gz ëª¨ë‘ ì²˜ë¦¬
        - ì••ì¶• í•´ì œ ê²°ê³¼ê°€ 'ìµœìƒìœ„ ë‹¨ì¼ í´ë”'ì¼ ê²½ìš°, ê·¸ í´ë”ë¥¼ í•œ ê²¹ í‰íƒ„í™”í•˜ì—¬
          í´ë” ë‚´ë¶€ì˜ íŒŒì¼/ë””ë ‰í„°ë¦¬ë¥¼ dest_dir ë°”ë¡œ ì•„ë˜ë¡œ ë³µì‚¬í•œë‹¤.
        - ì´í›„ dest ë‚´ ì‚°ì¶œë¬¼ì„ ì •ë¦¬í•˜ì—¬ chunks.jsonlì„ ë£¨íŠ¸ë¡œ ëª¨ìœ¼ê³  .readyë¥¼ ë³´ì •í•œë‹¤.
    """
    # E402 íšŒí”¼: í•¨ìˆ˜ ë‚´ë¶€ ë¡œì»¬ ì„í¬íŠ¸
    import tempfile, shutil

    dest = Path(dest_dir).expanduser()
    dest.mkdir(parents=True, exist_ok=True)

    repo = _repo()
    if not repo:
        _log("restore_latest: GITHUB_REPO ë¯¸ì„¤ì •")
        return False

    rel = _latest_release(repo)
    if not rel:
        return False

    name = rel.get("name") or rel.get("tag_name") or "(no-tag)"
    _log(f"ìµœì‹  ë¦´ë¦¬ìŠ¤: {name}")

    asset = _pick_best_asset(rel)
    if not asset:
        _log("ë¦´ë¦¬ìŠ¤ì— ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ìì‚°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    asset_name = str(asset.get("name") or "")
    _log(f"ìì‚° ë‹¤ìš´ë¡œë“œ: {asset_name}")
    data = _download_asset(asset)
    if not data:
        return False

    # ì„ì‹œ ë””ë ‰í„°ë¦¬ë¥¼ ì‚¬ìš©í•´ ì›ìì  êµì²´ì— ê°€ê¹ê²Œ ë³µì›
    with tempfile.TemporaryDirectory() as td:
        tmp = Path(td)

        # 1) ì••ì¶• í•´ì œ (.zip/.tar.gz/.tgz/.gz ìë™ íŒë³„)
        ok = _extract_auto(asset_name, data, tmp)
        if not ok:
            return False

        # 2) 'ìµœìƒìœ„ ë‹¨ì¼ í´ë”' ê°ì§€ â†’ í‰íƒ„í™” ëŒ€ìƒ ë£¨íŠ¸ ê²°ì •
        children = [
            p
            for p in tmp.iterdir()
            if p.name not in (".DS_Store",) and not p.name.startswith("__MACOSX")
        ]
        src_root = tmp
        if len(children) == 1 and children[0].is_dir():
            src_root = children[0]
            _log("í‰íƒ„í™” ì ìš©: ìµœìƒìœ„ í´ë” ë‚´ë¶€ë¥¼ ë£¨íŠ¸ë¡œ ìŠ¹ê²©")
            _log(f"ìŠ¹ê²© ëŒ€ìƒ í´ë”: '{src_root.name}'")

        # 3) ë³µì‚¬(ê¸°ì¡´ ë™ì¼ ê²½ë¡œëŠ” êµì²´). 'í´ë” ìì²´'ê°€ ì•„ë‹ˆë¼ 'í´ë” ë‚´ë¶€'ë¥¼ ë³µì‚¬í•œë‹¤.
        for p in src_root.iterdir():
            target = dest / p.name
            try:
                if target.exists():
                    if target.is_dir():
                        shutil.rmtree(target)
                    else:
                        target.unlink()
                if p.is_dir():
                    shutil.copytree(p, target)
                else:
                    shutil.copy2(p, target)
            except Exception as e:
                _log("íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨(ì¼ë¶€ í•­ëª©). ë‹¤ìŒ ë¼ì¸ì— ìƒì„¸ í‘œì‹œ.")
                _log(f"ì›ë³¸: {p.name} â†’ ëŒ€ìƒ: {target.name} â€” {type(e).__name__}: {e}")
                return False

    # 4) ì‚°ì¶œë¬¼ ì •ë¦¬(ê°•í™”): dest ì•ˆì—ì„œ chunks.jsonlì„ ë£¨íŠ¸ë¡œ ëª¨ìœ¼ê¸°
    def _size(p: Path) -> int:
        try:
            return p.stat().st_size
        except Exception:
            return 0

    def _decompress_gz(src: Path, dst: Path) -> bool:
        try:
            import gzip
            with gzip.open(src, "rb") as gf:
                data2 = gf.read()
            if not data2:
                return False
            dst.write_bytes(data2)
            return True
        except Exception as e:
            _log(f"gz í•´ì œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return False

    def _merge_dir_jsonl(chunk_dir: Path, out_file: Path) -> bool:
        """chunk_dir ì•ˆì˜ *.jsonlì„ ë¼ì¸ ë³´ì¡´ìœ¼ë¡œ ë³‘í•©í•œë‹¤."""
        try:
            bytes_written = 0
            tmp_out = out_file.with_suffix(".jsonl.tmp")
            if tmp_out.exists():
                tmp_out.unlink()
            with tmp_out.open("wb") as w:
                for p in sorted(chunk_dir.glob("*.jsonl")):
                    try:
                        with p.open("rb") as r:
                            while True:
                                buf = r.read(1024 * 1024)
                                if not buf:
                                    break
                                w.write(buf)
                                bytes_written += len(buf)
                    except Exception:
                        continue
            if bytes_written > 0:
                if out_file.exists():
                    out_file.unlink()
                tmp_out.replace(out_file)
                return True
            tmp_out.unlink(missing_ok=True)
            return False
        except Exception as e:
            _log(f"chunks/ ë³‘í•© ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return False

    target = dest / "chunks.jsonl"

    def _consolidate_to_target(root: Path, target_file: Path) -> bool:
        # ì´ë¯¸ ìœ íš¨í•˜ë©´ ë
        if target_file.exists() and _size(target_file) > 0:
            return True

        # a) ì •í™•ëª… ìš°ì„ : chunks.jsonl / chunks.jsonl.gz (ì„ì˜ ê¹Šì´)
        try:
            exact = [p for p in root.rglob("chunks.jsonl") if _size(p) > 0]
        except Exception:
            exact = []
        if exact:
            best = max(exact, key=_size)
            shutil.copy2(best, target_file)
            _log(f"exact chunks.jsonl ì‚¬ìš©: {best}")
            return True

        try:
            exact_gz = [p for p in root.rglob("chunks.jsonl.gz") if _size(p) > 0]
        except Exception:
            exact_gz = []
        if exact_gz:
            best_gz = max(exact_gz, key=_size)
            if _decompress_gz(best_gz, target_file):
                _log(f"exact chunks.jsonl.gz í•´ì œ: {best_gz}")
                return True

        # b) ë””ë ‰í„°ë¦¬ ë³‘í•©: */chunks/*.jsonl
        try:
            chunk_dirs = [d for d in root.rglob("chunks") if d.is_dir()]
        except Exception:
            chunk_dirs = []
        for d in chunk_dirs:
            if _merge_dir_jsonl(d, target_file):
                _log(f"ë””ë ‰í„°ë¦¬ ë³‘í•© ì‚¬ìš©: {d}")
                return True

        # c) ë²”ìš© íŒŒì¼: ì„ì˜ì˜ *.jsonl / *.jsonl.gz ì¤‘ ê°€ì¥ í° ê²ƒ ì„ íƒ
        try:
            any_jsonl = [p for p in root.rglob("*.jsonl") if _size(p) > 0]
        except Exception:
            any_jsonl = []
        if any_jsonl:
            best_any = max(any_jsonl, key=_size)
            shutil.copy2(best_any, target_file)
            _log(f"ì„ì˜ *.jsonl ì‚¬ìš©: {best_any}")
            return True

        try:
            any_gz = [p for p in root.rglob("*.jsonl.gz") if _size(p) > 0]
        except Exception:
            any_gz = []
        if any_gz:
            best_any_gz = max(any_gz, key=_size)
            if _decompress_gz(best_any_gz, target_file):
                _log(f"ì„ì˜ *.jsonl.gz í•´ì œ: {best_any_gz}")
                return True

        # ì‹¤íŒ¨ ì‹œ 0ë°”ì´íŠ¸ targetì´ ìˆìœ¼ë©´ ì œê±°
        if target_file.exists() and _size(target_file) == 0:
            target_file.unlink(missing_ok=True)
        return False

    ok_cons = _consolidate_to_target(dest, target)

    # ğŸ” ìµœì¢… í´ë°±: ë¦´ë¦¬ìŠ¤ ìì‚°ì´ chunks.jsonl.gz ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°, ì›ë³¸ ë°”ì´íŠ¸ë¡œ ì§ì ‘ í•´ì œ
    if not ok_cons and asset_name.lower().endswith(".gz") and data:
        try:
            import gzip
            raw = gzip.decompress(data)
            if raw:
                target.parent.mkdir(parents=True, exist_ok=True)
                target.write_bytes(raw)
                _log("ìì‚° ë°”ì´íŠ¸ ì§ì ‘ í•´ì œ: chunks.jsonl ìƒì„±(í´ë°±)")
                ok_cons = True
        except Exception as e:
            _log(f"í´ë°± í•´ì œ ì‹¤íŒ¨: {type(e).__name__}: {e}")

    if not ok_cons:
        _log("ì‚°ì¶œë¬¼ ì •ë¦¬ ì‹¤íŒ¨: chunks.jsonlì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # READY ë³´ì • ì—†ì´ ì¢…ë£Œ
        return False

    # 5) SSOT ë³´ì •: chunks.jsonlë§Œ ì¡´ì¬í•˜ê³  .readyê°€ ì—†ìœ¼ë©´ ìƒì„±
    try:
        chunks = dest / "chunks.jsonl"
        ready = dest / ".ready"
        if chunks.exists() and chunks.stat().st_size > 0 and not ready.exists():
            ready.write_text("ok", encoding="utf-8")
    except Exception:
        pass

    _log("ë³µì›ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return True
# ===== [06] PUBLIC API: restore_latest =======================================  # [06] END


# ===== [07] PUBLIC API: publish_backup =======================================  # [07] START
def publish_backup(persist_dir, keep: int = 5) -> bool:
    """
    ë¡œì»¬ ì¸ë±ìŠ¤ë¥¼ GitHub ë¦´ë¦¬ìŠ¤ì— ë°±ì—…í•œë‹¤.
    ì—…ë¡œë“œ: chunks.jsonl.gz, manifest.json
    íƒœê·¸: index-YYYYMMDD-HHMMSS (ê°€ëŠ¥í•˜ë©´ ê¸°ì¡´ manifest.build_id ì‚¬ìš©)
    ë³´ì¡´: 'index-' ì ‘ë‘ ë¦´ë¦¬ìŠ¤ ìµœê·¼ keepê°œë§Œ ë³´ì¡´
    """
    import io
    import json
    import gzip
    import time
    import hashlib
    import urllib.parse
    from typing import Any
    from pathlib import Path

    try:
        import requests  # í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ê°€ì •
    except Exception:
        _log("publish_backup: requests ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    def _as_path(p) -> Path:
        return p if isinstance(p, Path) else Path(str(p))

    def _sha256_file(p: Path) -> str:
        h = hashlib.sha256()
        with p.open("rb") as r:
            for chunk in iter(lambda: r.read(1024 * 1024), b""):
                h.update(chunk)
        return h.hexdigest()

    def _count_lines(p: Path) -> int:
        cnt = 0
        with p.open("rb") as f:
            for b in iter(lambda: f.read(1024 * 1024), b""):
                cnt += b.count(b"\n")
        return cnt

    # mypy-safe ìºìŠ¤íŒ… í—¬í¼
    def _as_int(v: Any, default: int) -> int:
        try:
            if v is None:
                return default
            if isinstance(v, bool):
                return int(v)
            if isinstance(v, int):
                return v
            if isinstance(v, float):
                return int(v)
            if isinstance(v, (bytes, bytearray)):
                s = v.decode(errors="ignore").strip()
                return int(s) if s else default
            if isinstance(v, str):
                s = v.strip()
                return int(s) if s else default
            # ìµœí›„ ìˆ˜ë‹¨
            return int(v)
        except Exception:
            return default

    base = _as_path(persist_dir)
    chunks = base / "chunks.jsonl"
    manifest = base / "manifest.json"

    if not chunks.exists() or chunks.stat().st_size == 0:
        _log("publish_backup: chunks.jsonl ì´ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return False

    repo = _repo()
    if not repo:
        _log("publish_backup: GITHUB_REPO ë¯¸ì„¤ì •")
        return False

    # manifest ë³´ì •/ìƒì„±
    mode = (_get_env("MAIC_INDEX_MODE", "STD") or "STD").upper()
    build_id = time.strftime("%Y%m%d-%H%M%S")
    manifest_obj: dict[str, Any] = {}
    try:
        if manifest.exists():
            manifest_obj = json.loads(manifest.read_text(encoding="utf-8") or "{}")
            build_id = str(manifest_obj.get("build_id") or build_id)
            manifest_obj["mode"] = str(manifest_obj.get("mode") or mode)
            manifest_obj["sha256"] = (
                manifest_obj.get("sha256") or _sha256_file(chunks)
            )
            manifest_obj["chunks"] = _as_int(manifest_obj.get("chunks"), _count_lines(chunks))
            manifest_obj["file"] = "chunks.jsonl"
            manifest_obj["persist_dir"] = str(base)
            manifest_obj["version"] = _as_int(manifest_obj.get("version"), 2)
        else:
            manifest_obj = {
                "build_id": build_id,
                "created_at": int(time.time()),
                "mode": mode,
                "file": "chunks.jsonl",
                "sha256": _sha256_file(chunks),
                "chunks": _count_lines(chunks),
                "persist_dir": str(base),
                "version": 2,
            }
            manifest.write_text(
                json.dumps(manifest_obj, ensure_ascii=False, indent=2),
                encoding="utf-8",
            )
    except Exception as e:
        _log(f"publish_backup: manifest ìƒì„±/ë³´ì • ì‹¤íŒ¨: {e}")
        return False

    tag = f"index-{build_id}"
    rel_name = f"{tag} ({mode})"

    session = requests.Session()
    session.headers.update(_headers())

    # ë¦´ë¦¬ìŠ¤ ìƒì„±/íšë“
    try:
        payload = {
            "tag_name": tag,
            "name": rel_name,
            "target_commitish": _branch(),
            "body": (
                "Automated index backup\n"
                f"- mode: {mode}\n"
                f"- chunks: {manifest_obj.get('chunks')}\n"
                f"- sha256: {manifest_obj.get('sha256')}\n"
            ),
            "draft": False,
            "prerelease": False,
        }
        r = session.post(
            f"{API}/repos/{repo}/releases", json=payload, timeout=30
        )
        if r.status_code not in (201, 422):
            _log(f"publish_backup: ë¦´ë¦¬ìŠ¤ ìƒì„± ì‹¤íŒ¨: {r.status_code} {r.text}")
            return False

        rel = r.json()
        if r.status_code == 422:
            rr = session.get(
                f"{API}/repos/{repo}/releases/tags/{tag}", timeout=15
            )
            if rr.status_code != 200:
                _log(
                    f"publish_backup: ê¸°ì¡´ ë¦´ë¦¬ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: "
                    f"{rr.status_code} {rr.text}"
                )
                return False
            rel = rr.json()

        upload_url_tpl = str(rel.get("upload_url") or "")
        upload_url = upload_url_tpl.split("{")[0]
        rel_id = rel.get("id")
        if not upload_url or not rel_id:
            _log("publish_backup: upload_url/release id íšë“ ì‹¤íŒ¨")
            return False
    except Exception as e:
        _log(f"publish_backup: ë¦´ë¦¬ìŠ¤ ìƒì„±/ì¡°íšŒ ì˜ˆì™¸: {e}")
        return False

    # ì—ì…‹ ì—…ë¡œë“œ
    try:
        # chunks.jsonl.gz
        gz = io.BytesIO()
        with gzip.GzipFile(fileobj=gz, mode="wb") as gzfp:
            gzfp.write(chunks.read_bytes())
        asset_name = "chunks.jsonl.gz"
        q = urllib.parse.urlencode({"name": asset_name})
        urla = f"{upload_url}?{q}"
        ra = session.post(
            urla,
            data=gz.getvalue(),
            headers=_upload_headers("application/gzip"),
            timeout=60,
        )
        if ra.status_code == 422:
            assets = session.get(
                f"{API}/repos/{repo}/releases/{rel_id}/assets", timeout=15
            ).json()
            old = next((a for a in assets if a.get("name") == asset_name), None)
            if old:
                aid = old.get("id")
                session.delete(f"{API}/releases/assets/{aid}", timeout=15)
                ra = session.post(
                    urla,
                    data=gz.getvalue(),
                    headers=_upload_headers("application/gzip"),
                    timeout=60,
                )
        if ra.status_code not in (201, 200):
            _log(f"publish_backup: chunks ì—…ë¡œë“œ ì‹¤íŒ¨: {ra.status_code} {ra.text}")
            return False

        # manifest.json
        m_name = "manifest.json"
        qm = urllib.parse.urlencode({"name": m_name})
        urlm = f"{upload_url}?{qm}"
        rm = session.post(
            urlm,
            data=json.dumps(manifest_obj, ensure_ascii=False).encode("utf-8"),
            headers=_upload_headers("application/json"),
            timeout=30,
        )
        if rm.status_code == 422:
            assets = session.get(
                f"{API}/repos/{repo}/releases/{rel_id}/assets", timeout=15
            ).json()
            old = next((a for a in assets if a.get("name") == m_name), None)
            if old:
                aid = old.get("id")
                session.delete(f"{API}/releases/assets/{aid}", timeout=15)
                rm = session.post(
                    urlm,
                    data=json.dumps(manifest_obj, ensure_ascii=False).encode("utf-8"),
                    headers=_upload_headers("application/json"),
                    timeout=30,
                )
        if rm.status_code not in (201, 200):
            _log(f"publish_backup: manifest ì—…ë¡œë“œ ì‹¤íŒ¨: {rm.status_code} {rm.text}")
            return False
    except Exception as e:
        _log(f"publish_backup: ì—ì…‹ ì—…ë¡œë“œ ì˜ˆì™¸: {e}")
        return False

    # ë³´ì¡´ ì •ì±… ì ìš©
    try:
        rels = []
        page = 1
        while True:
            rr = session.get(
                f"{API}/repos/{repo}/releases",
                params={"per_page": 100, "page": page},
                timeout=15,
            )
            if rr.status_code != 200:
                break
            batch = rr.json()
            if not isinstance(batch, list) or not batch:
                break
            rels.extend(batch)
            if len(batch) < 100:
                break
            page += 1

        # index-* ë§Œ
        index_rels = [
            r for r in rels
            if isinstance(r.get("tag_name"), str)
            and str(r.get("tag_name")).startswith("index-")
        ]
        index_rels.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        to_delete = index_rels[keep:] if keep > 0 else index_rels
        for item in to_delete:
            rid = item.get("id")
            tname = item.get("tag_name", "")
            if not rid:
                continue
            session.delete(f"{API}/repos/{repo}/releases/{rid}", timeout=15)
            if tname:
                session.delete(
                    f"{API}/repos/{repo}/git/refs/tags/{tname}", timeout=15
                )
    except Exception as e:
        _log(f"publish_backup: ë³´ì¡´ ì •ì±… ì˜ˆì™¸(ë¬´ì‹œ): {e}")

    _log(f"publish_backup: ì™„ë£Œ â€” tag={tag}, repo={repo}")
    return True
# ===== [07] PUBLIC API: publish_backup =======================================  # [07] END

