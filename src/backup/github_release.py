# ===== [01] IMPORTS & UTILS FALLBACK ========================================  # [01] START
from __future__ import annotations

import importlib
import io
import os
import shutil
import tempfile
import zipfile
from pathlib import Path
from types import ModuleType
from typing import Any, Dict, Optional, Protocol, Mapping, cast

import requests

# streamlitì€ ìˆì„ ìˆ˜ë„/ì—†ì„ ìˆ˜ë„ ìˆë‹¤.
# - mypy ì¶©ëŒ ë°©ì§€: st ë³€ìˆ˜ë¥¼ ë¨¼ì € Any|Noneë¡œ ì„ ì–¸ í›„, ëŸ°íƒ€ì„ì— ëª¨ë“ˆ/Noneì„ ëŒ€ì…
from typing import Any as _AnyForSt
st: _AnyForSt | None
try:
    import streamlit as _st_mod
    st = cast(_AnyForSt, _st_mod)
except Exception:
    st = None  # mypy OK: Optional[Any]

# ê³µìš© ìœ í‹¸: ëª¨ë“ˆ ë™ì  ì„í¬íŠ¸ í›„ ì¡´ì¬ í™•ì¸ + í´ë°± ì œê³µ
_utils: ModuleType | None
try:
    _utils = importlib.import_module("src.common.utils")
except Exception:
    _utils = None  # ëª¨ë“ˆ ìì²´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ


# --- ì •ì  ì¸í„°í˜ì´ìŠ¤(Protocol) ------------------------------------------------
class _LoggerProto(Protocol):
    def info(self, *a: Any, **k: Any) -> None: ...
    def warning(self, *a: Any, **k: Any) -> None: ...
    def error(self, *a: Any, **k: Any) -> None: ...


def get_secret(name: str, default: str = "") -> str:
    """Streamlit secrets â†’ env â†’ default ìˆœìœ¼ë¡œ ì¡°íšŒ(ë°˜í™˜ì€ í•­ìƒ str)."""
    # 1) src.common.utils.get_secret ìš°ì„ 
    if _utils is not None:
        func = getattr(_utils, "get_secret", None)
        if callable(func):
            try:
                val = func(name, default)
                # ì™¸ë¶€ utilì´ Optional/ë¹„ë¬¸ìì—´ì„ ì¤„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ ë³€í™˜
                if val is None:
                    return default
                return val if isinstance(val, str) else str(val)
            except Exception:
                pass

    # 2) streamlit.secrets (ì •ì  íƒ€ì… ê°€ë“œ + ë§¤í•‘ ìºìŠ¤íŒ…)
    try:
        if st is not None and hasattr(st, "secrets"):
            sec = cast(Mapping[str, Any], st.secrets)  # runtimeì€ Mapping ìœ ì‚¬ì²´
            v = sec.get(name, None)
            if v is not None:
                return v if isinstance(v, str) else str(v)
    except Exception:
        pass

    # 3) í™˜ê²½ë³€ìˆ˜ (Optional â†’ strë¡œ ê°•ì œ)
    env_v = os.getenv(name)
    return env_v if env_v is not None else default


def logger() -> _LoggerProto:
    """src.common.utils.logger()ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ _Logger(Protocol ì¤€ìˆ˜) ë°˜í™˜."""
    if _utils is not None:
        func = getattr(_utils, "logger", None)
        if callable(func):
            try:
                lg = func()
                # ì™¸ë¶€ êµ¬í˜„ì´ ë¬´ì—‡ì´ë“ , ìµœì†Œí•œ Protocol ì¶©ì¡± ë³´ì¥(duck typing)
                return cast(_LoggerProto, lg)
            except Exception:
                pass

    class _Logger:
        def info(self, *a: Any, **k: Any) -> None: ...
        def warning(self, *a: Any, **k: Any) -> None: ...
        def error(self, *a: Any, **k: Any) -> None: ...

    return _Logger()
# [01] END =====================================================================


# ===== [02] CONSTANTS & PUBLIC EXPORTS =======================================  # [02] START
API = "https://api.github.com"
__all__ = ["restore_latest", "get_latest_release"]
# [02] END =====================================================================


# [03] Release Publish: publish_backup() ========================================  # [03] START
def publish_backup(persist_dir: str | Path,
                   keep: int = 5) -> bool:
    """
    ë¡œì»¬ ì¸ë±ìŠ¤ë¥¼ GitHub ë¦´ë¦¬ìŠ¤ì— ë°±ì—…í•œë‹¤.
    - ì…ë ¥: persist_dir (ì˜ˆ: ~/.maic/persist)
    - ì—…ë¡œë“œ: chunks.jsonl.gz, manifest.json
    - íƒœê·¸ ê·œì¹™: index-YYYYMMDD-HHMMSS  (ê°€ëŠ¥í•˜ë©´ manifest.build_idë¥¼ ì‚¬ìš©)
    - ë³´ì¡´ ì •ì±…: 'index-' ì ‘ë‘ ë¦´ë¦¬ìŠ¤ ì¤‘ ìµœì‹  keepê°œë§Œ ë³´ì¡´(ê¸°ë³¸ 5ê°œ)
    ë°˜í™˜: ì„±ê³µ True / ì‹¤íŒ¨ False
    """
    # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©
    import os
    import io
    import json
    import gzip
    import time
    import math
    import shutil
    import urllib.parse
    from pathlib import Path

    try:
        import requests  # ì´ë¯¸ í”„ë¡œì íŠ¸ ì˜ì¡´ì„±ì— í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
    except Exception:
        print("publish_backup: requests ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    # ---- ìœ í‹¸ -----------------------------------------------------------------
    def _as_path(p) -> Path:
        return p if isinstance(p, Path) else Path(str(p))

    def _read_text(p: Path) -> str | None:
        try:
            return p.read_text(encoding="utf-8")
        except Exception:
            return None

    def _sha256_file(p: Path) -> str:
        import hashlib
        h = hashlib.sha256()
        with p.open("rb") as r:
            for chunk in iter(lambda: r.read(1024 * 1024), b""):
                h.update(chunk)
        return h.hexdigest()

    def _count_lines(p: Path, limit: int | None = None) -> int:
        # í° íŒŒì¼ì—ì„œë„ ë¹ ë¥´ê²Œ ë™ì‘í•˜ë„ë¡ ë²„í¼ë§
        cnt = 0
        try:
            with p.open("rb") as f:
                for b in iter(lambda: f.read(1024 * 1024), b""):
                    cnt += b.count(b"\n")
                    if limit and cnt >= limit:
                        return cnt
        except Exception:
            return 0
        return cnt

    def _gzip_bytes(data: bytes) -> bytes:
        buf = io.BytesIO()
        with gzip.GzipFile(fileobj=buf, mode="wb") as gz:
            gz.write(data)
        return buf.getvalue()

    def _git_headers(token: str) -> dict[str, str]:
        return {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28",
            "User-Agent": "maic-backup-bot",
        }

    def _upload_headers(token: str, content_type: str) -> dict[str, str]:
        h = _git_headers(token)
        h["Content-Type"] = content_type
        return h

    def _get_env(name: str, default: str = "") -> str:
        v = os.getenv(name)
        return v if isinstance(v, str) and v.strip() else default

    # ---- ì…ë ¥/í™˜ê²½ ìˆ˜ì§‘ --------------------------------------------------------
    base = _as_path(persist_dir)
    chunks = base / "chunks.jsonl"
    manifest = base / "manifest.json"

    if not chunks.exists() or chunks.stat().st_size == 0:
        print("publish_backup: chunks.jsonl ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return False

    # í™˜ê²½ ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ ëª¨ë“ˆ ìƒìˆ˜ ì‹œë„
    token = _get_env("GITHUB_TOKEN")
    repo = _get_env("GITHUB_REPO")
    branch = _get_env("GITHUB_BRANCH", "main")

    if not token or not repo:
        # ë¡œì»¬ ìƒìˆ˜ì— ì˜ì¡´(ìˆì„ ìˆ˜ë„, ì—†ì„ ìˆ˜ë„ ìˆìŒ)
        try:
            from src.backup.github_config import (  # ì„ íƒì 
                GITHUB_TOKEN as _TK,
                GITHUB_REPO as _RP,
                GITHUB_BRANCH as _BR,
            )
            token = token or _TK
            repo = repo or _RP
            branch = branch or _BR or branch
        except Exception:
            pass

    if not token or not repo:
        print("publish_backup: GITHUB_TOKEN/GITHUB_REPO ì„¤ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        return False

    api_base = f"https://api.github.com/repos/{repo}"
    session = requests.Session()
    session.headers.update(_git_headers(token))

    # ---- manifest êµ¬ì„±(ì—†ìœ¼ë©´ ìƒì„±) --------------------------------------------
    # mode í‘œì‹œëŠ” í™˜ê²½ë³€ìˆ˜/ê¸°ì¡´ manifestì—ì„œ ê°€ì ¸ì˜´(HQ ë²„íŠ¼ ëŒ€ì‘)
    mode = (_get_env("MAIC_INDEX_MODE", "STD") or "STD").upper()

    build_id = time.strftime("%Y%m%d-%H%M%S")  # ê¸°ë³¸ê°’
    manifest_obj: dict[str, object] = {}
    try:
        if manifest.exists():
            manifest_obj = json.loads(_read_text(manifest) or "{}")
            build_id = str(manifest_obj.get("build_id") or build_id)
            # ìµœì‹  ìŠ¤í‚¤ë§ˆ ë³´ì •
            manifest_obj["mode"] = str(manifest_obj.get("mode") or mode)
            manifest_obj["sha256"] = manifest_obj.get("sha256") or _sha256_file(chunks)
            manifest_obj["chunks"] = int(manifest_obj.get("chunks") or _count_lines(chunks))
            manifest_obj["file"] = "chunks.jsonl"
            manifest_obj["persist_dir"] = str(base)
            manifest_obj["version"] = int(manifest_obj.get("version") or 2)
        else:
            manifest_obj = {
                "build_id": build_id,
                "created_at": int(time.time()),
                "mode": mode,
                "file": "chunks.jsonl",
                "sha256": _sha256_file(chunks),
                "chunks": _count_lines(chunks),
                "persist_dir": str(base),
                "version": 2,
            }
            manifest.write_text(
                json.dumps(manifest_obj, ensure_ascii=False, indent=2),
                encoding="utf-8",
            )
    except Exception as e:
        print(f"publish_backup: manifest ìƒì„±/ë³´ì • ì‹¤íŒ¨: {e}")
        return False

    tag = f"index-{build_id}"
    rel_name = f"{tag} ({mode})"

    # ---- ë¦´ë¦¬ìŠ¤ ìƒì„± -----------------------------------------------------------
    try:
        payload = {
            "tag_name": tag,
            "name": rel_name,
            "target_commitish": branch,
            "body": (
                "Automated index backup\n"
                f"- mode: {mode}\n"
                f"- chunks: {manifest_obj.get('chunks')}\n"
                f"- sha256: {manifest_obj.get('sha256')}\n"
            ),
            "draft": False,
            "prerelease": False,
        }
        r = session.post(f"{api_base}/releases", json=payload, timeout=30)
        if r.status_code not in (201, 422):
            print(f"publish_backup: ë¦´ë¦¬ìŠ¤ ìƒì„± ì‹¤íŒ¨: {r.status_code} {r.text}")
            return False

        # 422(Unprocessable)ì¸ ê²½ìš°: ë™ì¼ íƒœê·¸ê°€ ì´ë¯¸ ìˆì„ ìˆ˜ ìˆìŒ â†’ í•´ë‹¹ ë¦´ë¦¬ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        rel = r.json()
        if r.status_code == 422:
            rr = session.get(f"{api_base}/releases/tags/{tag}", timeout=15)
            if rr.status_code != 200:
                print(f"publish_backup: ê¸°ì¡´ ë¦´ë¦¬ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {rr.status_code} {rr.text}")
                return False
            rel = rr.json()

        upload_url_tpl = rel.get("upload_url", "")
        upload_url = upload_url_tpl.split("{")[0]  # {?name,label} ì œê±°
        rel_id = rel.get("id")
        if not upload_url or not rel_id:
            print("publish_backup: upload_url ë˜ëŠ” release idë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        print(f"publish_backup: ë¦´ë¦¬ìŠ¤ ìƒì„±/ì¡°íšŒ ì˜ˆì™¸: {e}")
        return False

    # ---- ì—ì…‹ ì—…ë¡œë“œ(chunks.jsonl.gz, manifest.json) ---------------------------
    try:
        # gzip ì••ì¶•
        gz_data = _gzip_bytes(chunks.read_bytes())
        asset_name = "chunks.jsonl.gz"
        q = urllib.parse.urlencode({"name": asset_name})
        urla = f"{upload_url}?{q}"
        ra = session.post(
            urla,
            data=gz_data,
            headers=_upload_headers(token, "application/gzip"),
            timeout=60,
        )
        # ì´ë¯¸ ì¡´ì¬(422)í•˜ë©´ êµì²´ ìœ„í•´ ë¨¼ì € ì‚­ì œ í›„ ì¬ì—…ë¡œë“œ
        if ra.status_code == 422:
            # ê¸°ì¡´ ë™ì¼ ì´ë¦„ ì—ì…‹ id ì¡°íšŒ
            assets = session.get(f"{api_base}/releases/{rel_id}/assets", timeout=15).json()
            old = next((a for a in assets if a.get("name") == asset_name), None)
            if old:
                aid = old.get("id")
                session.delete(f"{api_base}/releases/assets/{aid}", timeout=15)
                ra = session.post(
                    urla,
                    data=gz_data,
                    headers=_upload_headers(token, "application/gzip"),
                    timeout=60,
                )
        if ra.status_code not in (201, 200):
            print(f"publish_backup: chunks ì—…ë¡œë“œ ì‹¤íŒ¨: {ra.status_code} {ra.text}")
            return False

        # manifest.json ì—…ë¡œë“œ(í…ìŠ¤íŠ¸)
        m_name = "manifest.json"
        qm = urllib.parse.urlencode({"name": m_name})
        urlm = f"{upload_url}?{qm}"
        rm = session.post(
            urlm,
            data=json.dumps(manifest_obj, ensure_ascii=False).encode("utf-8"),
            headers=_upload_headers(token, "application/json"),
            timeout=30,
        )
        if rm.status_code == 422:
            assets = session.get(f"{api_base}/releases/{rel_id}/assets", timeout=15).json()
            old = next((a for a in assets if a.get("name") == m_name), None)
            if old:
                aid = old.get("id")
                session.delete(f"{api_base}/releases/assets/{aid}", timeout=15)
                rm = session.post(
                    urlm,
                    data=json.dumps(manifest_obj, ensure_ascii=False).encode("utf-8"),
                    headers=_upload_headers(token, "application/json"),
                    timeout=30,
                )
        if rm.status_code not in (201, 200):
            print(f"publish_backup: manifest ì—…ë¡œë“œ ì‹¤íŒ¨: {rm.status_code} {rm.text}")
            return False
    except Exception as e:
        print(f"publish_backup: ì—ì…‹ ì—…ë¡œë“œ ì˜ˆì™¸: {e}")
        return False

    # ---- ë³´ì¡´ ì •ì±… ì ìš©(ìµœê·¼ keepê°œë§Œ ìœ ì§€) ------------------------------------
    try:
        rels = []
        page = 1
        while True:
            rr = session.get(
                f"{api_base}/releases",
                params={"per_page": 100, "page": page},
                timeout=15,
            )
            if rr.status_code != 200:
                break
            batch = rr.json()
            if not isinstance(batch, list) or not batch:
                break
            rels.extend(batch)
            if len(batch) < 100:
                break
            page += 1

        # index-* íƒœê·¸ë§Œ í•„í„°
        index_rels = [
            r for r in rels
            if isinstance(r.get("tag_name"), str) and r.get("tag_name", "").startswith("index-")
        ]
        # ìµœì‹ ìˆœ ì •ë ¬(created_at ê¸°ì¤€)
        index_rels.sort(key=lambda x: x.get("created_at", ""), reverse=True)

        # ë³´ì¡´ ëŒ€ìƒ ì œì™¸í•˜ê³  ì‚­ì œ ëª©ë¡ ì‚°ì¶œ
        to_delete = index_rels[keep:] if keep > 0 else index_rels
        for item in to_delete:
            rid = item.get("id")
            tname = item.get("tag_name", "")
            if not rid:
                continue
            # ë¦´ë¦¬ìŠ¤ ì‚­ì œ
            session.delete(f"{api_base}/releases/{rid}", timeout=15)
            # íƒœê·¸ë„ ì •ë¦¬(ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ)
            if tname:
                tr = session.delete(f"{api_base}/git/refs/tags/{tname}", timeout=15)
                if tr.status_code not in (204, 200, 404):
                    print(f"publish_backup: íƒœê·¸ ì‚­ì œ ì‹¤íŒ¨: {tname} {tr.status_code}")
    except Exception as e:
        # ë³´ì¡´ ì •ì±… ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ â†’ True ë°˜í™˜ì€ ìœ ì§€
        print(f"publish_backup: ë³´ì¡´ ì •ì±… ì ìš© ì¤‘ ì˜ˆì™¸(ë¬´ì‹œ): {e}")

    print(f"publish_backup: ì™„ë£Œ â€” tag={tag}, repo={repo}")
    return True
# [03] END ======================================================================



# ===== [04] RELEASE DISCOVERY =================================================  # [04] START
def _latest_release(repo: str) -> Optional[dict]:
    """ê°€ì¥ ìµœì‹  ë¦´ë¦¬ìŠ¤ë¥¼ ì¡°íšŒ. ì‹¤íŒ¨ ì‹œ None."""
    if not repo:
        _log("GITHUB_REPOê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    url = f"{API}/repos/{repo}/releases/latest"
    try:
        r = requests.get(url, headers=_headers(), timeout=15)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        _log(f"ìµœì‹  ë¦´ë¦¬ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return None


def get_latest_release(repo: Optional[str] = None) -> Optional[dict]:
    """
    PUBLIC API: ìµœì‹  GitHub ë¦´ë¦¬ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    - repo ì¸ìê°€ ì—†ìœ¼ë©´ secrets/envì˜ GITHUB_REPOë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ìš”ì²­/íŒŒì‹± ì‹¤íŒ¨ ì‹œ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤(ì˜ˆì™¸ ë°œìƒí•˜ì§€ ì•ŠìŒ).
    """
    target = (repo or _repo()).strip()
    rel = _latest_release(target)
    if rel is None:
        return None
    # ìµœì†Œ í•„ë“œ ì •ê·œí™”(í˜¸ì¶œì¸¡ í¸ì˜)
    if "tag_name" not in rel and "name" in rel:
        rel["tag_name"] = rel.get("name")
    return rel


def _pick_best_asset(rel: dict) -> Optional[dict]:
    """ë¦´ë¦¬ìŠ¤ ìì‚° ì¤‘ ìš°ì„ ìˆœìœ„(.zip > .tar.gz > ì²« ë²ˆì§¸)ë¥¼ ì„ íƒ."""
    assets = rel.get("assets") or []
    if not assets:
        return None
    for a in assets:
        if str(a.get("name", "")).lower().endswith(".zip"):
            return a
    for a in assets:
        if str(a.get("name", "")).lower().endswith(".tar.gz"):
            return a
    return assets[0] if assets else None
# [04] END


# ===== [05] ASSET DOWNLOAD & EXTRACT =========================================  # [05] START
def _download_asset(asset: dict) -> Optional[bytes]:
    """GitHub ë¦´ë¦¬ìŠ¤ ìì‚°ì„ ë‚´ë ¤ë°›ì•„ ë°”ì´íŠ¸ë¡œ ë°˜í™˜. ì‹¤íŒ¨ ì‹œ None."""
    url = asset.get("url") or asset.get("browser_download_url")
    if not url:
        return None
    try:
        r = requests.get(url, headers=_headers(binary=True), timeout=60)
        r.raise_for_status()
        return r.content
    except Exception as e:
        _log(f"ìì‚° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return None


def _extract_zip(data: bytes, dest_dir: Path) -> bool:
    """ZIP ë°”ì´íŠ¸ë¥¼ dest_dirì— í’€ê¸°. ì„±ê³µ True/ì‹¤íŒ¨ False."""
    try:
        with zipfile.ZipFile(io.BytesIO(data)) as zf:
            zf.extractall(dest_dir)
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(zip): {type(e).__name__}: {e}")
        return False


def _extract_targz(data: bytes, dest_dir: Path) -> bool:
    """TAR.GZ / TGZ ë°”ì´íŠ¸ë¥¼ dest_dirì— í’€ê¸°."""
    try:
        import tarfile
        with tarfile.open(fileobj=io.BytesIO(data), mode="r:gz") as tf:
            tf.extractall(dest_dir)
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(tar.gz): {type(e).__name__}: {e}")
        return False


def _extract_gz_to_file(asset_name: str, data: bytes, dest_dir: Path) -> bool:
    """ë‹¨ì¼ .gz(ì˜ˆ: chunks.jsonl.gz)ë¥¼ dest_dir/<basename>ìœ¼ë¡œ í’€ê¸°."""
    try:
        import gzip  # ì§€ì—­ ì„í¬íŠ¸ë¡œ ìƒë‹¨ êµ¬íš ë³€ê²½ ë¶ˆí•„ìš”
        base = asset_name[:-3] if asset_name.lower().endswith(".gz") else asset_name
        out_path = dest_dir / base
        with gzip.GzipFile(fileobj=io.BytesIO(data), mode="rb") as gf:
            out_path.write_bytes(gf.read())
        return True
    except Exception as e:
        _log(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨(gz): {type(e).__name__}: {e}")
        return False


def _extract_auto(asset_name: str, data: bytes, dest_dir: Path) -> bool:
    """ìì‚° ì´ë¦„ìœ¼ë¡œ í˜•ì‹ì„ ìœ ì¶”í•˜ì—¬ ì ì ˆíˆ í•´ì œ."""
    n = (asset_name or "").lower()
    if n.endswith(".zip"):
        return _extract_zip(data, dest_dir)
    if n.endswith(".tar.gz") or n.endswith(".tgz"):
        return _extract_targz(data, dest_dir)
    if n.endswith(".gz"):
        return _extract_gz_to_file(asset_name, data, dest_dir)
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹: zip ì‹œë„(ì‹¤íŒ¨ ì‹œ False)
    return _extract_zip(data, dest_dir)
# [05] END =====================================================================

# ===== [06] PUBLIC API: restore_latest =======================================  # [06] START
def restore_latest(dest_dir: str | Path) -> bool:
    """ìµœì‹  GitHub Releaseì—ì„œ ì•„í‹°íŒ©íŠ¸ë¥¼ ë‚´ë ¤ë°›ì•„ dest_dirì— ë³µì›.

    ë°˜í™˜:
        ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False (ì˜ˆì™¸ëŠ” ì˜¬ë¦¬ì§€ ì•ŠìŒ)

    ë¹„ê³ :
        - .zip/.tar.gz/.tgz/.gz ëª¨ë‘ ì²˜ë¦¬
        - ì••ì¶• í•´ì œ ê²°ê³¼ê°€ 'ìµœìƒìœ„ ë‹¨ì¼ í´ë”'ì¼ ê²½ìš°, ê·¸ í´ë”ë¥¼ í•œ ê²¹ í‰íƒ„í™”í•˜ì—¬
          í´ë” ë‚´ë¶€ì˜ íŒŒì¼/ë””ë ‰í„°ë¦¬ë¥¼ dest_dir ë°”ë¡œ ì•„ë˜ë¡œ ë³µì‚¬í•œë‹¤.
        - ì´í›„ dest ë‚´ ì‚°ì¶œë¬¼ì„ ì •ë¦¬í•˜ì—¬ chunks.jsonlì„ ë£¨íŠ¸ë¡œ ëª¨ìœ¼ê³  .readyë¥¼ ë³´ì •í•œë‹¤.
    """
    dest = Path(dest_dir).expanduser()
    dest.mkdir(parents=True, exist_ok=True)

    repo = _repo()
    if not repo:
        _log("restore_latest: GITHUB_REPO ë¯¸ì„¤ì •")
        return False

    rel = _latest_release(repo)
    if not rel:
        return False

    name = rel.get("name") or rel.get("tag_name") or "(no-tag)"
    _log(f"ìµœì‹  ë¦´ë¦¬ìŠ¤: {name}")

    asset = _pick_best_asset(rel)
    if not asset:
        _log("ë¦´ë¦¬ìŠ¤ì— ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ìì‚°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    asset_name = str(asset.get("name") or "")
    _log(f"ìì‚° ë‹¤ìš´ë¡œë“œ: {asset_name}")
    data = _download_asset(asset)
    if not data:
        return False

    # ì„ì‹œ ë””ë ‰í„°ë¦¬ë¥¼ ì‚¬ìš©í•´ ì›ìì  êµì²´ì— ê°€ê¹ê²Œ ë³µì›
    with tempfile.TemporaryDirectory() as td:
        tmp = Path(td)

        # 1) ì••ì¶• í•´ì œ (.zip/.tar.gz/.tgz/.gz ìë™ íŒë³„)
        ok = _extract_auto(asset_name, data, tmp)
        if not ok:
            return False

        # 2) 'ìµœìƒìœ„ ë‹¨ì¼ í´ë”' ê°ì§€ â†’ í‰íƒ„í™” ëŒ€ìƒ ë£¨íŠ¸ ê²°ì •
        children = [
            p
            for p in tmp.iterdir()
            if p.name not in (".DS_Store",) and not p.name.startswith("__MACOSX")
        ]
        src_root = tmp
        if len(children) == 1 and children[0].is_dir():
            src_root = children[0]
            _log("í‰íƒ„í™” ì ìš©: ìµœìƒìœ„ í´ë” ë‚´ë¶€ë¥¼ ë£¨íŠ¸ë¡œ ìŠ¹ê²©")
            _log(f"ìŠ¹ê²© ëŒ€ìƒ í´ë”: '{src_root.name}'")

        # 3) ë³µì‚¬(ê¸°ì¡´ ë™ì¼ ê²½ë¡œëŠ” êµì²´). 'í´ë” ìì²´'ê°€ ì•„ë‹ˆë¼ 'í´ë” ë‚´ë¶€'ë¥¼ ë³µì‚¬í•œë‹¤.
        for p in src_root.iterdir():
            target = dest / p.name
            try:
                if target.exists():
                    if target.is_dir():
                        shutil.rmtree(target)
                    else:
                        target.unlink()
                if p.is_dir():
                    shutil.copytree(p, target)
                else:
                    shutil.copy2(p, target)
            except Exception as e:
                _log("íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨(ì¼ë¶€ í•­ëª©). ë‹¤ìŒ ë¼ì¸ì— ìƒì„¸ í‘œì‹œ.")
                _log(f"ì›ë³¸: {p.name} â†’ ëŒ€ìƒ: {target.name} â€” {type(e).__name__}: {e}")
                return False

    # 4) ì‚°ì¶œë¬¼ ì •ë¦¬(ê°•í™”): dest ì•ˆì—ì„œ chunks.jsonlì„ ë£¨íŠ¸ë¡œ ëª¨ìœ¼ê¸°
    def _size(p: Path) -> int:
        try:
            return p.stat().st_size
        except Exception:
            return 0

    def _decompress_gz(src: Path, dst: Path) -> bool:
        try:
            import gzip
            with gzip.open(src, "rb") as gf:
                data2 = gf.read()
            if not data2:
                return False
            dst.write_bytes(data2)
            return True
        except Exception as e:
            _log(f"gz í•´ì œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return False

    def _merge_dir_jsonl(chunk_dir: Path, out_file: Path) -> bool:
        """chunk_dir ì•ˆì˜ *.jsonlì„ ë¼ì¸ ë³´ì¡´ìœ¼ë¡œ ë³‘í•©í•œë‹¤."""
        try:
            bytes_written = 0
            tmp_out = out_file.with_suffix(".jsonl.tmp")
            if tmp_out.exists():
                tmp_out.unlink()
            with tmp_out.open("wb") as w:
                for p in sorted(chunk_dir.glob("*.jsonl")):
                    try:
                        with p.open("rb") as r:
                            while True:
                                buf = r.read(1024 * 1024)
                                if not buf:
                                    break
                                w.write(buf)
                                bytes_written += len(buf)
                    except Exception:
                        continue
            if bytes_written > 0:
                if out_file.exists():
                    out_file.unlink()
                tmp_out.replace(out_file)
                return True
            tmp_out.unlink(missing_ok=True)
            return False
        except Exception as e:
            _log(f"chunks/ ë³‘í•© ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return False

    target = dest / "chunks.jsonl"

    def _consolidate_to_target(root: Path, target_file: Path) -> bool:
        # ì´ë¯¸ ìœ íš¨í•˜ë©´ ë
        if target_file.exists() and _size(target_file) > 0:
            return True

        # a) ì •í™•ëª… ìš°ì„ : chunks.jsonl / chunks.jsonl.gz (ì„ì˜ ê¹Šì´)
        try:
            exact = [p for p in root.rglob("chunks.jsonl") if _size(p) > 0]
        except Exception:
            exact = []
        if exact:
            best = max(exact, key=_size)
            shutil.copy2(best, target_file)
            _log(f"exact chunks.jsonl ì‚¬ìš©: {best}")
            return True

        try:
            exact_gz = [p for p in root.rglob("chunks.jsonl.gz") if _size(p) > 0]
        except Exception:
            exact_gz = []
        if exact_gz:
            best_gz = max(exact_gz, key=_size)
            if _decompress_gz(best_gz, target_file):
                _log(f"exact chunks.jsonl.gz í•´ì œ: {best_gz}")
                return True

        # b) ë””ë ‰í„°ë¦¬ ë³‘í•©: */chunks/*.jsonl
        try:
            chunk_dirs = [d for d in root.rglob("chunks") if d.is_dir()]
        except Exception:
            chunk_dirs = []
        for d in chunk_dirs:
            if _merge_dir_jsonl(d, target_file):
                _log(f"ë””ë ‰í„°ë¦¬ ë³‘í•© ì‚¬ìš©: {d}")
                return True

        # c) ë²”ìš© íŒŒì¼: ì„ì˜ì˜ *.jsonl / *.jsonl.gz ì¤‘ ê°€ì¥ í° ê²ƒ ì„ íƒ
        try:
            any_jsonl = [p for p in root.rglob("*.jsonl") if _size(p) > 0]
        except Exception:
            any_jsonl = []
        if any_jsonl:
            best_any = max(any_jsonl, key=_size)
            shutil.copy2(best_any, target_file)
            _log(f"ì„ì˜ *.jsonl ì‚¬ìš©: {best_any}")
            return True

        try:
            any_gz = [p for p in root.rglob("*.jsonl.gz") if _size(p) > 0]
        except Exception:
            any_gz = []
        if any_gz:
            best_any_gz = max(any_gz, key=_size)
            if _decompress_gz(best_any_gz, target_file):
                _log(f"ì„ì˜ *.jsonl.gz í•´ì œ: {best_any_gz}")
                return True

        # ì‹¤íŒ¨ ì‹œ 0ë°”ì´íŠ¸ targetì´ ìˆìœ¼ë©´ ì œê±°
        if target_file.exists() and _size(target_file) == 0:
            target_file.unlink(missing_ok=True)
        return False

    ok_cons = _consolidate_to_target(dest, target)

    # ğŸ” ìµœì¢… í´ë°±: ë¦´ë¦¬ìŠ¤ ìì‚°ì´ chunks.jsonl.gz ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°, ì›ë³¸ ë°”ì´íŠ¸ë¡œ ì§ì ‘ í•´ì œ
    if not ok_cons and asset_name.lower().endswith(".gz") and data:
        try:
            import gzip
            raw = gzip.decompress(data)
            if raw:
                target.parent.mkdir(parents=True, exist_ok=True)
                target.write_bytes(raw)
                _log("ìì‚° ë°”ì´íŠ¸ ì§ì ‘ í•´ì œ: chunks.jsonl ìƒì„±(í´ë°±)")
                ok_cons = True
        except Exception as e:
            _log(f"í´ë°± í•´ì œ ì‹¤íŒ¨: {type(e).__name__}: {e}")

    if not ok_cons:
        _log("ì‚°ì¶œë¬¼ ì •ë¦¬ ì‹¤íŒ¨: chunks.jsonlì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # READY ë³´ì • ì—†ì´ ì¢…ë£Œ
        return False

    # 5) SSOT ë³´ì •: chunks.jsonlë§Œ ì¡´ì¬í•˜ê³  .readyê°€ ì—†ìœ¼ë©´ ìƒì„±
    try:
        chunks = dest / "chunks.jsonl"
        ready = dest / ".ready"
        if chunks.exists() and chunks.stat().st_size > 0 and not ready.exists():
            ready.write_text("ok", encoding="utf-8")
    except Exception:
        pass

    _log("ë³µì›ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return True
# ===== [06] PUBLIC API: restore_latest =======================================  # [06] END


# ===== [07] PUBLIC API: publish_backup =======================================  # [07] START
def publish_backup(persist_dir: str | Path, tag_prefix: str = "index") -> dict | None:
    """
    í˜„ì¬ ë¡œì»¬ ì¸ë±ìŠ¤(PERSIST_DIR/chunks.jsonl)ë¥¼ GitHub Releaseë¡œ ë°±ì—… ë°œí–‰.
    - ìƒˆ íƒœê·¸: {tag_prefix}-YYYYMMDD-HHMMSS-{shortid}
    - ìì‚°: chunks.jsonl.gz (+ manifest.json ìˆìœ¼ë©´ í•¨ê»˜ ì—…ë¡œë“œ)
    ë°˜í™˜: {"tag": "...", "release_id": int, "assets": [{"name":..., "size":...}, ...]} ë˜ëŠ” None
    """
    from pathlib import Path
    import os
    import io
    import gzip
    import json
    import datetime
    import requests
    import hashlib

    dest = Path(persist_dir).expanduser()
    src = dest / "chunks.jsonl"
    if not (src.exists() and src.stat().st_size > 0):
        _log("publish_backup: chunks.jsonlì´ ì—†ê±°ë‚˜ 0B")
        return None

    repo = _repo()
    if not repo:
        _log("publish_backup: GITHUB_REPO ë¯¸ì„¤ì •")
        return None

    token = str(globals().get("GITHUB_TOKEN", "")).strip() or os.getenv("GITHUB_TOKEN", "").strip()
    if not token:
        _log("publish_backup: GITHUB_TOKEN ë¯¸ì„¤ì •")
        return None

    # sha1 â†’ shortid
    h = hashlib.sha1()
    with src.open("rb") as r:
        for b in iter(lambda: r.read(1024 * 1024), b""):
            h.update(b)
    shortid = h.hexdigest()[:8]

    # ìì‚° gzip
    gz_name = "chunks.jsonl.gz"
    buf = io.BytesIO()
    with gzip.GzipFile(filename="chunks.jsonl", mode="wb", fileobj=buf) as z:
        z.write(src.read_bytes())
    data_bytes = buf.getvalue()

    # ë¦´ë¦¬ìŠ¤ ìƒì„±
    now = datetime.datetime.utcnow().strftime("%Y%m%d-%H%M%S")
    tag = f"{tag_prefix}-{now}-{shortid}"
    api = f"https://api.github.com/repos/{repo}/releases"
    headers = {"Authorization": f"token {token}", "Accept": "application/vnd.github+json"}

    body = ""
    mf = dest / "manifest.json"
    if mf.exists():
        try:
            body = mf.read_text(encoding="utf-8")[:4000]  # ë³¸ë¬¸ ê¸¸ì´ ì œí•œ ëŒ€ë¹„
        except Exception:
            body = ""

    payload = {"tag_name": tag, "name": tag, "body": body, "draft": False, "prerelease": False}
    try:
        res = requests.post(api, headers=headers, data=json.dumps(payload), timeout=30)
        if res.status_code >= 300:
            _log(f"publish_backup: ë¦´ë¦¬ìŠ¤ ìƒì„± ì‹¤íŒ¨ {res.status_code} {res.text[:200]}")
            return None
        rel = res.json()
        upload_url = rel.get("upload_url", "")
        upload_url = upload_url.split("{", 1)[0]  # https://uploads.github.com/.../assets
        rid = int(rel.get("id") or 0)
    except Exception as e:
        _log(f"publish_backup: ìš”ì²­ ì‹¤íŒ¨ â€” {type(e).__name__}: {e}")
        return None

    assets_meta = []

    # ìì‚° ì—…ë¡œë“œ í•¨ìˆ˜
    def _upload(name: str, content_type: str, data: bytes) -> bool:
        try:
            up_url = f"{upload_url}?name={name}"
            up_headers = {
                "Authorization": f"token {token}",
                "Content-Type": content_type,
                "Accept": "application/vnd.github+json",
            }
            res2 = requests.post(up_url, headers=up_headers, data=data, timeout=120)
            if res2.status_code >= 300:
                _log(f"publish_backup: {name} ì—…ë¡œë“œ ì‹¤íŒ¨ {res2.status_code} {res2.text[:200]}")
                return False
            assets_meta.append({"name": name, "size": len(data)})
            return True
        except Exception as e:
            _log(f"publish_backup: {name} ì—…ë¡œë“œ ì˜ˆì™¸ â€” {type(e).__name__}: {e}")
            return False

    # chunks.jsonl.gz
    if not _upload(gz_name, "application/gzip", data_bytes):
        return None

    # manifest.json ë™ë°˜ ì—…ë¡œë“œ(ìˆìœ¼ë©´)
    if mf.exists():
        try:
            _upload("manifest.json", "application/json", mf.read_bytes())
        except Exception:
            pass

    _log(f"ë°±ì—… ë°œí–‰ ì™„ë£Œ: {tag} (assets: {[a['name'] for a in assets_meta]})")
    return {"tag": tag, "release_id": rid, "assets": assets_meta}
# ===== [07] PUBLIC API: publish_backup =======================================  # [07] END
